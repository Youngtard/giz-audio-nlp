{"cells":[{"metadata":{},"cell_type":"markdown","source":"1. Convert additional data to wav format resampled at 22.5kHz - code available on local computer\n2. Convert audio to hdf5 format"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\nimport h5py\nimport librosa\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport os\nimport time\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n\ndef random_seed(SEED = SEED):    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nrandom_seed()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/giz-data/giz_data/Train.csv\")\nsample_submission = pd.read_csv(\"../input/giz-data/giz_data/SampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ORIGINAL_AUDIO_PATH = \"../input/giz-data/giz_data/audio_files/\"\n# Named AdditionalUtterances\nADDITIONAL_AUDIO_PATH_1 = \"../input/giz-data/AdditionalUtterancesConverted/AdditionalUtterancesConverted/\"\nADDITIONAL_AUDIO_PATH_2 = \"../input/giz-data/AdditionalUtterancesConverted_2/AdditionalUtterancesConverted_2/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# match fnames in dataframes to audio path\ndf_train[\"fn\"] = df_train.fn.str.replace(\"audio_files/\", ORIGINAL_AUDIO_PATH)\nsample_submission[\"fn\"] = sample_submission.fn.str.replace(\"audio_files/\", ORIGINAL_AUDIO_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = sample_submission[[\"fn\"]].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"fold\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -1, train, others == eval set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sss = StratifiedShuffleSplit(n_splits = 5, random_state = SEED, test_size = 0.2)\nkfold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = SEED)\n\nfor fold, (train_idx, valid_idx) in enumerate(kfold.split(df_train, df_train.label.values)):\n    \n#     df_train.loc[valid_idx, \"fold\"] = df_train.loc[valid_idx, \"fold\"].apply(lambda row: fold + 1 if row == -1 else row)\n    df_train.loc[valid_idx, \"fold\"] = fold + 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fold.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Porblems with additional audio data like similar voicings/duplicates, sr was initially different, file type was initially mp3\ndf_train[\"weight\"] = 0.65","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add additional audio details to existing dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path_1 = ADDITIONAL_AUDIO_PATH_1 + \"latest_keywords\"\nadditional_labels_1 = os.listdir(base_path_1)\n\nbase_path_2 = ADDITIONAL_AUDIO_PATH_2 + \"nlp_keywords\"\nadditional_labels_2 = os.listdir(base_path_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_fns = []\nnew_labels = []\n# batch = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in additional_labels_1:\n    samples_in_label = os.listdir(os.path.join(base_path_1, label))\n    \n    for sample in samples_in_label:\n        file_path = os.path.join(base_path_1, label + \"/\" + sample)\n        new_fns.append(file_path)\n        new_labels.append(label)\n#         batch.append(1)\n\n        \nfor label in additional_labels_2:\n    samples_in_label = os.listdir(os.path.join(base_path_2, label))\n    \n    for sample in samples_in_label:\n        file_path = os.path.join(base_path_2, label + \"/\" + sample)\n        new_fns.append(file_path)\n        new_labels.append(label)\n#         batch.append(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = np.asarray([new_fns, new_labels]).T\ndf_new = pd.DataFrame(df_new, columns = [\"fn\", \"label\"])\n\n# df_new[\"batch\"] = batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # kept all batch 2 data\n\n# df_new[\"id_\"] = df_new.fn.apply(lambda row: row.split(\"/\")[-1].split(\"_\")[0])\n# to_keep = df_new[df_new.batch == 1][[\"label\", \"id_\"]].drop_duplicates().index.append(df_new[df_new.batch == 2].index)\n# df_new = df_new.iloc[to_keep].reset_index(drop = True)\n# df_new = df_new.drop(\"id_\", axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_new.drop(\"batch\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use all for training - identified by -1\ndf_new[\"fold\"] = -1\ndf_new[\"weight\"] = 0.35","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New df to be used for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concat\ndf_train_new = pd.concat([df_train, df_new], axis = 0).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort\ndf_train_new = df_train_new.sort_values(\"label\").reset_index(drop = True)\n# Shuffle\ndf_train_new = df_train_new.sample(frac=1, random_state = 42).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_new[\"audio_id\"] = df_train_new.fn.apply(lambda row: row.split(\"/\")[-1][:-4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[\"audio_id\"] = df_test.fn.apply(lambda row: row.split(\"/\")[-1][:-4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encode labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick Label encoding\ndf_train_new[\"target\"] = df_train_new.label.astype(\"category\").cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_new.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CONST"},{"metadata":{"trusted":true},"cell_type":"code","source":"# config/arguments/parameters\n\nSAMPLE_RATE = 16000 #22050\nCLIP_SAMPLES = SAMPLE_RATE * 3 #SAMPLE_RATE * 30\nNUM_CLASSES = 193 #10\n# WORKSPACE_PATH = , HDF5 PATH SHOULD BE DYNAMIC\nMIXUP = False\nWINDOW_SIZE = 512 #1024# 2048\nHOP_SIZE = 160 #320\nMEL_BINS = 64\nFMIN = 50#20\nFMAX = SAMPLE_RATE // 2.0\nFREEZE_BASE = True\n\nBATCH_SIZE = 64 # * 2 if mixup\n\nAUGMENTATION = ['mixup']\n\nMODEL_TYPE = \"Transfer_Cnn14\" #\"Cnn14_DecisionLevelAtt\"  #\"Cnn14\"\nPRETRAINED_CHECKPOINT_PATH = \"../input/pretrained-pann/Cnn14_16k_mAP0.438.pth\"\n# PRETRAINED_CHECKPOINT_PATH = \"../input/resnet38-0-434/ResNet38_0_434.pth\"\n# PRETRAINED_CHECKPOINT_PATH = \"../input/wavegram-logmel-cnn14-0-439/Wavegram_Logmel_Cnn14_0_439.pth\"\n\n\nfrom torch import cuda\nDEVICE = 'cuda' if cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def traverse_folder(fd):\n    paths = []\n    names = []\n\n    for root, dirs, files in os.walk(fd):\n        for name in files:\n            filepath = os.path.join(root, name)\n            names.append(name)\n            paths.append(filepath)\n\n    return names, paths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folder(fd):\n    if not os.path.exists(fd):\n        os.makedirs(fd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_truncate_sequence(x, max_len):\n    if len(x) < max_len:\n        return np.concatenate((x, np.zeros(max_len - len(x))))\n    else:\n        return x[0 : max_len]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def float32_to_int16(x):\n    # assert np.max(np.abs(x)) <= 1.\n    if np.max(np.abs(x)) > 1.:\n        x /= np.max(np.abs(x))\n    return (x * 32767.).astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def int16_to_float32(x):\n    return (x / 32767.).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_one_hot(k, classes_num):\n    target = np.zeros(classes_num)\n    target[k] = 1\n    return target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def move_data_to_device(x, device):\n    if 'float' in str(x.dtype):\n        x = torch.Tensor(x)\n    elif 'int' in str(x.dtype):\n        x = torch.LongTensor(x)\n    else:\n        return x\n\n    return x.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def collate_fn(list_data_dict):\n    \"\"\"Collate data.\n    Args:\n      list_data_dict, e.g., [{'audio_name': str, 'waveform': (clip_samples,), ...}, \n                             {'audio_name': str, 'waveform': (clip_samples,), ...},\n                             ...]\n    Returns:\n      np_data_dict, dict, e.g.,\n          {'audio_name': (batch_size,), 'waveform': (batch_size, clip_samples), ...}\n    \"\"\"\n    np_data_dict = {}\n    \n    for key in list_data_dict[0].keys():\n        np_data_dict[key] = np.array([data_dict[key] for data_dict in list_data_dict])\n    \n    return np_data_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pack_audio_files_to_hdf5(df, workspace, train = True):\n\n#     # Paths\n#     audios_dir = df.fn.values\n\n    if train:\n        packed_hdf5_path = os.path.join(workspace, 'features', 'waveform_train.h5')\n    else:\n        packed_hdf5_path = os.path.join(workspace, 'features', 'waveform_test.h5')\n     \n    # Create folder\n    create_folder(os.path.dirname(packed_hdf5_path))\n    \n    audio_names = df.audio_id.values\n    audio_paths = df.fn.values\n\n    if train:\n        meta_dict = {\n            'audio_name': np.array(audio_names), \n            'audio_path': np.array(audio_paths), \n            'target': df.target.values, \n            'fold': df.fold.values\n        }\n    else:\n        meta_dict = {\n            'audio_name': np.array(audio_names), \n            'audio_path': np.array(audio_paths), \n        }        \n        \n    \n    audios_num = len(meta_dict['audio_name'])\n\n    feature_time = time.time()\n    with h5py.File(packed_hdf5_path, 'w') as hf:\n        hf.create_dataset(\n            name='audio_name', \n            shape=(audios_num,), \n            dtype='S80')\n\n        hf.create_dataset(\n            name='waveform', \n            shape=(audios_num, CLIP_SAMPLES), \n            dtype=np.int16)\n\n        if train:\n            hf.create_dataset(\n                name='target', \n                shape=(audios_num,), \n                dtype=np.int32)\n\n            hf.create_dataset(\n                name='fold', \n                shape=(audios_num,), \n                dtype=np.int16)\n \n        for n in range(audios_num):\n            print(n)\n            audio_name = meta_dict['audio_name'][n]\n            audio_path = meta_dict['audio_path'][n]\n            \n            if train:\n                fold = meta_dict['fold'][n]\n            \n            (audio, fs) = librosa.core.load(audio_path, sr = SAMPLE_RATE, mono=True, res_type = \"kaiser_fast\")#sr=SAMPLE_RATE\n\n            audio = pad_truncate_sequence(audio, CLIP_SAMPLES)\n\n            hf['audio_name'][n] = audio_name.encode()\n            hf['waveform'][n] = float32_to_int16(audio)\n            \n            if train:\n                hf['target'][n] = meta_dict['target'][n]\n                hf['fold'][n] = meta_dict['fold'][n]\n\n    print('Write hdf5 to {}'.format(packed_hdf5_path))\n    print('Time: {:.3f} s'.format(time.time() - feature_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pack_audio_files_to_hdf5(df_train_new, \"./\", train = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pack_audio_files_to_hdf5(df_test, \"./\", train = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AudioDataset(object):\n    def __init__(self, train = True):\n        \"\"\"This class takes the meta of an audio clip as input, and return \n        the waveform and target of the audio clip. This class is used by DataLoader. \n        Args:\n          clip_samples: int\n          classes_num: int\n        \"\"\"\n        self.train = train\n#         pass\n    \n    def __getitem__(self, meta):\n        \"\"\"Load waveform and target of an audio clip.\n        \n        Args:\n          meta: {\n            'audio_name': str, \n            'hdf5_path': str, \n            'index_in_hdf5': int}\n        Returns: \n          data_dict: {\n            'audio_name': str, \n            'waveform': (clip_samples,), \n            'target': (classes_num,)}\n        \"\"\"\n        hdf5_path = meta['hdf5_path']\n        index_in_hdf5 = meta['index_in_hdf5']\n\n        with h5py.File(hdf5_path, 'r') as hf:\n            audio_name = hf['audio_name'][index_in_hdf5].decode()\n            waveform = int16_to_float32(hf['waveform'][index_in_hdf5])\n            if self.train:\n                target = hf['target'][index_in_hdf5].astype(np.int32)\n\n        if self.train:\n            data_dict = {\n                'audio_name': audio_name, \n                'waveform': waveform, \n                'target': target\n            }\n            \n            return data_dict\n        else:\n            data_dict = {\n                'audio_name': audio_name, \n                'waveform': waveform\n            }            \n            \n            return data_dict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainSampler1(object):\n    def __init__(self, hdf5_path, holdout_fold, batch_size, random_seed = 1234):\n        \"\"\"Balanced sampler. Generate batch meta for training.\n        \n        Args:\n          indexes_hdf5_path: string\n          batch_size: int\n          black_list_csv: string\n          random_seed: int\n        \"\"\"\n        # super(TrainSampler, self).__init__(indexes_hdf5_path, batch_size, \n            # random_seed)\n\n        self.hdf5_path = hdf5_path\n        self.batch_size = batch_size\n        self.random_state = np.random.RandomState(random_seed)\n\n        with h5py.File(hdf5_path, 'r') as hf:\n            self.folds = hf['fold'][:].astype(np.float32)\n\n        self.indexes = np.where(self.folds != int(holdout_fold))[0]\n        self.audios_num = len(self.indexes)\n        # self.validate_audio_indexes = np.where(self.folds == int(holdout_fold))[0]\n        \n        # self.indexes = np.arange(self.audios_num)\n            \n        # Shuffle indexes\n        self.random_state.shuffle(self.indexes)\n        \n        self.pointer = 0\n        \n    def __len__(self):\n        #len of loader\n        print(len(self.indexes))\n        return int(np.ceil(len(self.indexes) / self.batch_size))\n\n    def __iter__(self):\n        \"\"\"Generate batch meta for training. \n        \n        Returns:\n          batch_meta: e.g.: [\n            {'audio_name': 'YfWBzCRl6LUs.wav', \n             'hdf5_path': 'xx/balanced_train.h5', \n             'index_in_hdf5': 15734, \n             'target': [0, 1, 0, 0, ...]}, \n            ...]\n        \"\"\"\n        batch_size = self.batch_size\n\n        while True:\n            batch_meta = []\n            i = 0\n            while i < batch_size:\n                index = self.indexes[self.pointer]\n                self.pointer += 1\n\n                # Shuffle indexes and reset pointer\n                if self.pointer >= self.audios_num:\n                    self.pointer = 0\n                    self.random_state.shuffle(self.indexes)\n                \n                batch_meta.append({\n                    'hdf5_path': self.hdf5_path, \n                    'index_in_hdf5': self.indexes[self.pointer]\n                })\n                i += 1\n\n            yield batch_meta\n\n    def state_dict(self):\n        state = {\n            'indexes': self.indexes,\n            'pointer': self.pointer\n        }\n        return state\n            \n    def load_state_dict(self, state):\n        self.indexes = state['indexes']\n        self.pointer = state['pointer']\n\n\nclass EvaluateSampler(object):\n    def __init__(self, hdf5_path, holdout_fold, batch_size, random_seed=1234):\n        \"\"\"Balanced sampler. Generate batch meta for training.\n        \n        Args:\n          indexes_hdf5_path: string\n          batch_size: int\n          black_list_csv: string\n          random_seed: int\n        \"\"\"\n        # super(TrainSampler, self).__init__(indexes_hdf5_path, batch_size, \n            # random_seed)\n\n        self.hdf5_path = hdf5_path\n        self.batch_size = batch_size\n\n        with h5py.File(hdf5_path, 'r') as hf:\n            self.folds = hf['fold'][:].astype(np.float32)\n\n        self.indexes = np.where(self.folds == int(holdout_fold))[0]\n        self.audios_num = len(self.indexes)\n        \n    def __len__(self):\n        #len of loader\n        return int(np.ceil(len(self.indexes) / self.batch_size))        \n        \n    def __iter__(self):\n        \"\"\"Generate batch meta for training. \n        \n        Returns:\n          batch_meta: e.g.: [\n            {'audio_name': 'YfWBzCRl6LUs.wav', \n             'hdf5_path': 'xx/balanced_train.h5', \n             'index_in_hdf5': 15734, \n             'target': [0, 1, 0, 0, ...]}, \n            ...]\n        \"\"\"\n        batch_size = self.batch_size\n        pointer = 0\n\n        while pointer < self.audios_num:\n            batch_indexes = np.arange(pointer, \n                min(pointer + batch_size, self.audios_num))\n\n            batch_meta = []\n\n            for i in batch_indexes:\n                batch_meta.append({\n                    'hdf5_path': self.hdf5_path, \n                    'index_in_hdf5': self.indexes[i]\n                })\n\n            pointer += batch_size\n            yield batch_meta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestSampler(object):\n    def __init__(self, hdf5_path, batch_size, random_seed=1234):\n        \"\"\"Balanced sampler. Generate batch meta for training.\n        \n        Args:\n          indexes_hdf5_path: string\n          batch_size: int\n          black_list_csv: string\n          random_seed: int\n        \"\"\"\n        # super(TrainSampler, self).__init__(indexes_hdf5_path, batch_size, \n            # random_seed)\n\n        self.hdf5_path = hdf5_path\n        self.batch_size = batch_size\n\n        with h5py.File(hdf5_path, 'r') as hf:\n            self.audio_name = hf['audio_name'][:]\n\n        self.indexes = [i for i in range(len(self.audio_name))]\n        self.audios_num = len(self.indexes)\n        \n    def __len__(self):\n        #len of loader\n        return int(np.ceil(len(self.indexes) / self.batch_size))        \n        \n    def __iter__(self):\n        \"\"\"Generate batch meta for training. \n        \n        Returns:\n          batch_meta: e.g.: [\n            {'audio_name': 'YfWBzCRl6LUs.wav', \n             'hdf5_path': 'xx/balanced_train.h5', \n             'index_in_hdf5': 15734, \n             'target': [0, 1, 0, 0, ...]}, \n            ...]\n        \"\"\"\n        batch_size = self.batch_size\n        pointer = 0\n\n        while pointer < self.audios_num:\n            batch_indexes = np.arange(pointer, \n                min(pointer + batch_size, self.audios_num))\n\n            batch_meta = []\n\n            for i in batch_indexes:\n                batch_meta.append({\n                    'hdf5_path': self.hdf5_path, \n                    'index_in_hdf5': self.indexes[i]\n                })\n\n            pointer += batch_size\n            yield batch_meta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainSampler2(object):\n    def __init__(self, hdf5_path, holdout_fold, batch_size, random_seed=1234):\n        \"\"\"Balanced sampler. Generate batch meta for training.\n        \n        Args:\n          indexes_hdf5_path: string\n          batch_size: int\n          black_list_csv: string\n          random_seed: int\n        \"\"\"\n        # super(TrainSampler, self).__init__(indexes_hdf5_path, batch_size, \n            # random_seed)\n\n        self.hdf5_path = hdf5_path\n        self.batch_size = batch_size\n\n        with h5py.File(hdf5_path, 'r') as hf:\n            self.folds = hf['fold'][:].astype(np.float32)\n\n        self.indexes = np.where(self.folds != int(holdout_fold))[0]\n        self.audios_num = len(self.indexes)\n        \n    def __len__(self):\n        #len of loader\n        return int(np.ceil(len(self.indexes) / self.batch_size))        \n        \n    def __iter__(self):\n        \"\"\"Generate batch meta for training. \n        \n        Returns:\n          batch_meta: e.g.: [\n            {'audio_name': 'YfWBzCRl6LUs.wav', \n             'hdf5_path': 'xx/balanced_train.h5', \n             'index_in_hdf5': 15734, \n             'target': [0, 1, 0, 0, ...]}, \n            ...]\n        \"\"\"\n        batch_size = self.batch_size\n        pointer = 0\n\n        while pointer < self.audios_num:\n            batch_indexes = np.arange(pointer, \n                min(pointer + batch_size, self.audios_num))\n\n            batch_meta = []\n\n            for i in batch_indexes:\n                batch_meta.append({\n                    'hdf5_path': self.hdf5_path, \n                    'index_in_hdf5': self.indexes[i]\n                })\n\n            pointer += batch_size\n            yield batch_meta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = AudioDataset()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_sampler1 = TrainSampler1(\n    hdf5_path=\"./features/waveform_train.h5\", \n    holdout_fold = 1, \n    batch_size = BATCH_SIZE * 2 if MIXUP else BATCH_SIZE)\n\ntrain_sampler2 = TrainSampler2(\n    hdf5_path=\"./features/waveform_train.h5\", \n    holdout_fold = 1, \n    batch_size = BATCH_SIZE * 2 if MIXUP else BATCH_SIZE)\n\nvalid_sampler = EvaluateSampler(\n    hdf5_path = \"./features/waveform_train.h5\", \n    holdout_fold = 1, \n    batch_size = BATCH_SIZE)\n\n# Data loader\ntrain_loader1 = DataLoader(dataset=dataset, \n    batch_sampler=train_sampler1, \n                          collate_fn=collate_fn, \n    num_workers=2, pin_memory=True)\n\ntrain_loader2 = DataLoader(dataset=dataset, \n    batch_sampler=train_sampler2, \n                          collate_fn=collate_fn, \n    num_workers=2, pin_memory=True)\n\nvalidate_loader = DataLoader(dataset=dataset, \n    batch_sampler=valid_sampler, collate_fn=collate_fn, \n    num_workers=2, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Data loader\n# train_loader = DataLoader(dataset=dataset, \n#     num_workers=2, pin_memory=True)\n\n# validate_loader = DataLoader(dataset=dataset, \n#     num_workers=2, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = []\nidds = []\nbtchs = []\nii = 0\nfor btch in train_loader2:\n    for i, t in zip(btch[\"audio_name\"], btch[\"target\"]):\n        ts.append(t)\n        idds.append(i)\n#     print(btch[\"target\"])\n    btchs.append(btch[\"target\"])\n    if ii == 67:\n        break\n    ii += 1\n    \n# sample_data_iter = iter(validate_loader)\n# sample_output = sample_data_iter.next()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(ts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(btchs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(idds).duplicated().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(idds)[pd.Series(idds).duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for b in btchs:\n    print(len(b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_data_iter = iter(train_loader2)\nsample_output = sample_data_iter.next()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dt in train_loader:\n    print(dt)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, dt in enumerate(train_loader, 1):\n    print(dt)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_output[\"waveform\"][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    Model = eval(MODEL_TYPE)\n    model = Model(SAMPLE_RATE, WINDOW_SIZE, HOP_SIZE, MEL_BINS, FMIN, FMAX, NUM_CLASSES, FREEZE_BASE)\n    model.load_from_pretrain(PRETRAINED_CHECKPOINT_PATH)\n    model.to(DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train()\n\nfor key in sample_output.keys():\n    sample_output[key] = move_data_to_device(sample_output[key], DEVICE) \n\ns_out = model(sample_output[\"waveform\"])\ns_target = sample_output[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_out[\"clipwise_output\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn.CrossEntropyLoss().to(DEVICE)(s_out[\"clipwise_output\"], s_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# s_out['clipwise_output'].T[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_output[\"target\"] * s_out['clipwise_output'].T[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_output['target'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func(s_out, {'target': sample_output['target']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, j in enumerate(train_loader, 0):\n    print(i)\n#     break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mixup Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Mixup(object):\n    def __init__(self, mixup_alpha, random_seed=1234):\n        \"\"\"Mixup coefficient generator.\n        \"\"\"\n        self.mixup_alpha = mixup_alpha\n        self.random_state = np.random.RandomState(random_seed)\n\n    def get_lambda(self, batch_size):\n        \"\"\"Get mixup random coefficients.\n        Args:\n          batch_size: int\n        Returns:\n          mixup_lambdas: (batch_size,)\n        \"\"\"\n        mixup_lambdas = []\n        for n in range(0, batch_size, 2):\n            lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n            mixup_lambdas.append(lam)\n            mixup_lambdas.append(1. - lam)\n\n        return np.array(mixup_lambdas)\n\n\ndef do_mixup(x, mixup_lambda):\n    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes \n    (1, 3, 5, ...).\n    Args:\n      x: (batch_size * 2, ...)\n      mixup_lambda: (batch_size * 2,)\n    Returns:\n      out: (batch_size, ...)\n    \"\"\"\n    out = (x[0 :: 2].transpose(0, -1) * mixup_lambda[0 :: 2] + \\\n        x[1 :: 2].transpose(0, -1) * mixup_lambda[1 :: 2]).transpose(0, -1)\n    return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchlibrosa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchlibrosa.stft import Spectrogram, LogmelFilterBank\nfrom torchlibrosa.augmentation import SpecAugmentation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_layer(layer):\n    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n    nn.init.xavier_uniform_(layer.weight)\n \n    if hasattr(layer, 'bias'):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_bn(bn):\n    \"\"\"Initialize a Batchnorm layer. \"\"\"\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        \n        super(ConvBlock, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=in_channels, \n                              out_channels=out_channels,\n                              kernel_size=(3, 3), stride=(1, 1),\n                              padding=(1, 1), bias=False)\n                              \n        self.conv2 = nn.Conv2d(in_channels=out_channels, \n                              out_channels=out_channels,\n                              kernel_size=(3, 3), stride=(1, 1),\n                              padding=(1, 1), bias=False)\n                              \n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.init_weight()\n        \n    def init_weight(self):\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n\n        \n    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n        \n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        x = F.relu_(self.bn2(self.conv2(x)))\n        if pool_type == 'max':\n            x = F.max_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg':\n            x = F.avg_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg+max':\n            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n            x2 = F.max_pool2d(x, kernel_size=pool_size)\n            x = x1 + x2\n        else:\n            raise Exception('Incorrect argument!')\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Cnn14(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n        fmax, classes_num):\n        \n        super(Cnn14, self).__init__()\n\n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(time_drop_width=32, time_stripes_num=2, \n            freq_drop_width=8, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(64)\n\n        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n\n        self.fc1 = nn.Linear(2048, 2048, bias=True)\n        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n        \n        self.init_weight()\n\n    def init_weight(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        init_layer(self.fc_audioset)\n \n    def forward(self, input, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n\n        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n        \n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n        \n        if self.training:\n            x = self.spec_augmenter(x)\n\n        # Mixup on spectrogram\n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n\n        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = torch.mean(x, dim=3)\n        \n        (x1, _) = torch.max(x, dim=2)\n        x2 = torch.mean(x, dim=2)\n        x = x1 + x2\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu_(self.fc1(x))\n        embedding = F.dropout(x, p=0.5, training=self.training)\n        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n        \n        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n\n        return output_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvPreWavBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        \n        super(ConvPreWavBlock, self).__init__()\n        \n        self.conv1 = nn.Conv1d(in_channels=in_channels, \n                              out_channels=out_channels,\n                              kernel_size=3, stride=1,\n                              padding=1, bias=False)\n                              \n        self.conv2 = nn.Conv1d(in_channels=out_channels, \n                              out_channels=out_channels,\n                              kernel_size=3, stride=1, dilation=2, \n                              padding=2, bias=False)\n                              \n        self.bn1 = nn.BatchNorm1d(out_channels)\n        self.bn2 = nn.BatchNorm1d(out_channels)\n\n        self.init_weight()\n        \n    def init_weight(self):\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n\n        \n    def forward(self, input, pool_size):\n        \n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        x = F.relu_(self.bn2(self.conv2(x)))\n        x = F.max_pool1d(x, kernel_size=pool_size)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Wavegram_Logmel_Cnn14(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n        fmax, classes_num):\n        \n        super(Wavegram_Logmel_Cnn14, self).__init__()\n\n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n\n        self.pre_conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=11, stride=5, padding=5, bias=False)\n        self.pre_bn0 = nn.BatchNorm1d(64)\n        self.pre_block1 = ConvPreWavBlock(64, 64)\n        self.pre_block2 = ConvPreWavBlock(64, 128)\n        self.pre_block3 = ConvPreWavBlock(128, 128)\n        self.pre_block4 = ConvBlock(in_channels=4, out_channels=64)\n\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n            freq_drop_width=8, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(64)\n\n        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n        self.conv_block2 = ConvBlock(in_channels=128, out_channels=128)\n        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n\n        self.fc1 = nn.Linear(2048, 2048, bias=True)\n        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n        \n        self.init_weight()\n\n    def init_weight(self):\n        init_layer(self.pre_conv0)\n        init_bn(self.pre_bn0)\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        init_layer(self.fc_audioset)\n \n    def forward(self, input, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n\n        # Wavegram\n        a1 = F.relu_(self.pre_bn0(self.pre_conv0(input[:, None, :])))\n        a1 = self.pre_block1(a1, pool_size=4)\n        a1 = self.pre_block2(a1, pool_size=4)\n        a1 = self.pre_block3(a1, pool_size=4)\n        a1 = a1.reshape((a1.shape[0], -1, 32, a1.shape[-1])).transpose(2, 3)\n        a1 = self.pre_block4(a1, pool_size=(2, 1))\n\n        # Log mel spectrogram\n        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n        \n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n\n        if self.training:\n            x = self.spec_augmenter(x)\n\n        # Mixup on spectrogram\n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n            a1 = do_mixup(a1, mixup_lambda)\n        \n        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n\n        # Concatenate Wavegram and Log mel spectrogram along the channel dimension\n        x = torch.cat((x, a1), dim=1)\n\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = torch.mean(x, dim=3)\n        \n        (x1, _) = torch.max(x, dim=2)\n        x2 = torch.mean(x, dim=2)\n        x = x1 + x2\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu_(self.fc1(x))\n        embedding = F.dropout(x, p=0.5, training=self.training)\n        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n        \n        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n\n        return output_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Cnn14_16k(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, fmax, classes_num):\n        \n        super(Cnn14_16k, self).__init__() \n\n        assert sample_rate == 16000\n        assert window_size == 512\n        assert hop_size == 160\n        assert mel_bins == 64\n        assert fmin == 50\n        assert fmax == 8000\n\n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n            freq_drop_width=8, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(64)\n\n        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n\n        self.fc1 = nn.Linear(2048, 2048, bias=True)\n        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n        \n        self.init_weight()\n\n    def init_weight(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        init_layer(self.fc_audioset)\n \n    def forward(self, input, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n\n        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n\n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n        \n        if self.training:\n            x = self.spec_augmenter(x)\n\n        # Mixup on spectrogram\n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n        \n        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = torch.mean(x, dim=3)\n        \n        (x1, _) = torch.max(x, dim=2)\n        x2 = torch.mean(x, dim=2)\n        x = x1 + x2\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu_(self.fc1(x))\n        embedding = F.dropout(x, p=0.5, training=self.training)\n        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n        \n        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n\n        return output_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Custom model head for task"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Transfer_Cnn14(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n        fmax, classes_num, freeze_base):\n        \"\"\"Classifier for a new task using pretrained Cnn14 as a sub module.\n        \"\"\"\n        super(Transfer_Cnn14, self).__init__()\n        audioset_classes_num = 527\n        \n        self.base = Cnn14_16k(sample_rate, window_size, hop_size, mel_bins, fmin, \n            fmax, audioset_classes_num)\n\n        # Transfer to another task layer\n        self.fc_transfer = nn.Linear(2048, classes_num, bias=True)\n\n        if freeze_base:\n            # Freeze AudioSet pretrained layers\n            for param in self.base.parameters():\n                param.requires_grad = False\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.fc_transfer)\n\n    def load_from_pretrain(self, pretrained_checkpoint_path):\n        checkpoint = torch.load(pretrained_checkpoint_path)\n        self.base.load_state_dict(checkpoint['model'])\n\n    def forward(self, input, mixup_lambda=None):\n        \"\"\"Input: (batch_size, data_length)\n        \"\"\"\n        output_dict = self.base(input, mixup_lambda)\n        embedding = output_dict['embedding']\n\n        clipwise_output =  torch.log_softmax(self.fc_transfer(embedding), dim=-1)\n        output_dict['clipwise_output'] = clipwise_output\n \n        return output_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LEARNING_RATE = 1e-3\n# OPTIMIZER = optim.AdamW(model.parameters(), lr = LEARNING_RATE, weight_decay = 0.1)\nif 'mixup' in AUGMENTATION:\n    MIXUP_AUGMENTER = Mixup(mixup_alpha = 1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward(model, generator, return_input=False, \n    return_target=False):\n    \"\"\"Forward data to a model.\n    \n    Args: \n      model: object\n      generator: object\n      return_input: bool\n      return_target: bool\n    Returns:\n      audio_name: (audios_num,)\n      clipwise_output: (audios_num, classes_num)\n      (ifexist) segmentwise_output: (audios_num, segments_num, classes_num)\n      (ifexist) framewise_output: (audios_num, frames_num, classes_num)\n      (optional) return_input: (audios_num, segment_samples)\n      (optional) return_target: (audios_num, classes_num)\n    \"\"\"\n    def append_to_dict(dict, key, value):\n        if key in dict.keys():\n            dict[key].append(value)\n        else:\n            dict[key] = [value]\n\n    output_dict = {}\n    device = next(model.parameters()).device\n\n    # Forward data to a model in mini-batches\n    for n, batch_data_dict in enumerate(generator):\n        batch_waveform = move_data_to_device(batch_data_dict['waveform'], device)\n        \n        with torch.no_grad():\n            model.eval()\n            batch_output = model(batch_waveform)\n\n        append_to_dict(output_dict, 'audio_name', batch_data_dict['audio_name'])\n\n        append_to_dict(output_dict, 'clipwise_output', \n            batch_output['clipwise_output'].data.cpu().numpy()\n                      )\n            \n        if return_input:\n            append_to_dict(output_dict, 'waveform', batch_data_dict['waveform'])\n            \n        if return_target:\n            if 'target' in batch_data_dict.keys():\n                append_to_dict(output_dict, 'target', batch_data_dict['target'])\n\n    for key in output_dict.keys():\n        output_dict[key] = np.concatenate(output_dict[key], axis=0)\n\n    return output_dict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_accuracy(y_true, y_score):\n    N = y_true.shape[0]\n    accuracy = np.sum(np.argmax(y_true, axis=-1) == np.argmax(y_score, axis=-1)) / N\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Iteration {iteration}', end = '\\n')\nprint(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\n# iteration = 0\n# stop_iteration = 1000\nvalidation_cycle = 100\n# checkpoint_cycle = 50\n\n# Train on mini batches\ndef train_evaluate(model, train_loader, valid_loader, loss_func, device, stop_iteration):\n    iteration = 1\n    lr = 1e-3\n    optimizer = optim.AdamW(model.parameters(), lr = lr, weight_decay = 0.1)\n#     optimizer = optim.Adam(model.parameters(), lr = lr)\n#     optimizer = optim.SGD(model.parameters(), lr = lr, weight_decay = 0.01)\n#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, 1700)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps = 680)\n    \n    \n    for batch_data_dict in train_loader:\n    \n        print(f'Iteration {iteration}', end = ', ')\n\n        # Move data to device as tensor\n        for key in batch_data_dict.keys():\n            batch_data_dict[key] = move_data_to_device(batch_data_dict[key], device)\n\n        # Train\n        model.train()\n\n        batch_output_dict = model(batch_data_dict['waveform'], None)\n        batch_target_dict = {'target': batch_data_dict['target']}\n\n        # loss\n        loss = loss_func(batch_output_dict[\"clipwise_output\"], batch_target_dict[\"target\"])\n        print(f'loss: {loss}', end = '\\n')\n\n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n#         scheduler.step()\n\n        # Evaluate\n        if iteration % validation_cycle == 0 and iteration > 0:\n\n            output_dict = forward(model, valid_loader, return_target=True)\n            clipwise_output = output_dict['clipwise_output']    # (audios_num, classes_num)\n            target = output_dict['target']    # (audios_num, classes_num)           \n\n#             cm = metrics.confusion_matrix(np.argmax(target, axis=-1),\n#                                             np.argmax(clipwise_output, axis=-1),\n#                                             labels=None)\n\n#             val_accuracy = calculate_accuracy(target, clipwise_output)\n            val_loss = loss_func(torch.tensor(clipwise_output), torch.tensor(target, dtype = torch.long))\n#             print(f', val_acc:{val_accuracy}')\n            print(f'val_loss: {val_loss}')\n#             print('Confusion Matrix:')\n#             print(cm)\n#             print(idx_to_lb)\n\n#         # Save model \n#         if iteration % checkpoint_cycle == 0 and iteration > 0:\n#             checkpoint = {\n#                 'iteration': iteration, \n#                 'model': model.state_dict()}\n\n#             checkpoint_name = f'{iteration}_iterations.pth'\n#             checkpoint_path = os.path.join(checkpoints_dir, checkpoint_name)\n\n#             torch.save(checkpoint, checkpoint_path)\n#             print(f'Model saved at {checkpoint_name}')\n\n#         print()\n\n        # Stop learning\n        if iteration == stop_iteration:\n            break \n\n        iteration += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold in range(1, 4):\n    \n    train_sampler = TrainSampler1(\n        hdf5_path=\"./features/waveform_train.h5\", \n        holdout_fold = fold, \n        batch_size = 32 * 2 if MIXUP else BATCH_SIZE\n    )\n\n    valid_sampler = EvaluateSampler(\n        hdf5_path = \"./features/waveform_train.h5\", \n        holdout_fold = fold, \n        batch_size = 32\n    )\n    \n    test_sampler = TestSampler(\n        hdf5_path = \"./features/waveform_test.h5\", \n        batch_size = 32\n    )   \n    \n    train_dataset = AudioDataset()\n    test_dataset = AudioDataset(train = False)\n\n    # Data loader\n    train_loader = DataLoader(dataset = train_dataset, \n        batch_sampler=train_sampler, collate_fn=collate_fn, \n        num_workers=2, pin_memory=True\n    )\n\n    valid_loader = DataLoader(dataset = train_dataset, \n        batch_sampler=valid_sampler, collate_fn=collate_fn, \n        num_workers=2, pin_memory=True\n    )    \n    \n    test_loader = DataLoader(dataset = test_dataset, \n        batch_sampler=test_sampler, collate_fn=collate_fn, \n        num_workers=2, pin_memory=True\n    )  \n    \n#     if 'mixup' in AUGMENTATION:\n#         MIXUP_AUGMENTER = Mixup(mixup_alpha = 1.)\n\n    random_seed()\n    Model = eval(MODEL_TYPE)\n    model = Model(SAMPLE_RATE, WINDOW_SIZE, HOP_SIZE, MEL_BINS, FMIN, FMAX, NUM_CLASSES, FREEZE_BASE)\n    model.load_from_pretrain(PRETRAINED_CHECKPOINT_PATH)\n    model.to(DEVICE)\n    \n    loss_func = nn.CrossEntropyLoss().to(DEVICE)\n\n    print(f'Fold {fold}')\n    \n    train_evaluate(model = model, train_loader = train_loader, valid_loader = valid_loader, loss_func = loss_func, device = DEVICE, stop_iteration = 2000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean([1.67, 1.34, 1.54]) - 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"1.66, 1.66, 1.63, 1700it","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MIXUP OR MIXUP IN AUGMENTATION","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_evaluate(model, train_loader, valid_loader, test_loader, loss_fn, lr, epochs, warm_up_prop, device, n_samples_train, n_samples_val, fold):\n    \n    num_training_steps = epochs * len(train_loader)\n    num_warmup_steps = int(warm_up_prop * num_training_steps)\n    print(num_training_steps, num_warmup_steps)\n    optimizer = optim.AdamW(model.parameters(), lr = lr, weight_decay = 0.1)\n#     optimizer = Lookahead(optimizer)\n#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps) #using transformers library\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps = num_training_steps)\n\n    validation_loss = 0\n    validation_loss_min = np.Inf\n    \n    \n    for epoch in range(epochs):\n        # Train\n        model.train()\n        start_time = time.time()\n        \n        batch_losses_train = []\n        n_correct = 0\n        \n        for batch_index, batch_data in enumerate(train_loader, 0):\n            \n# #             if 'mixup' in augmentation:\n#             if MIXUP:\n#                 batch_data['mixup_lambda'] = MIXUP_AUGMENTER.get_lambda(len(batch_data['waveform']))            \n            \n            # batch_data type is a dict\n            # Send input to device\n            for key in batch_data.keys():\n                batch_data[key] = move_data_to_device(batch_data[key], device)\n            \n            waveforms = batch_data[\"waveform\"]\n            targets = batch_data[\"target\"]\n#             mixup_lambda = batch_data[\"mixup_lambda\"]\n#             targets = do_mixup(batch_data[\"target\"], mixup_lambda).long()\n            \n            random_seed()\n            outputs = model(waveforms)\n            clipwise_outputs = outputs[\"clipwise_output\"]\n\n            loss = loss_fn(clipwise_outputs, targets)            \n            batch_losses_train.append(loss.item())\n            _, preds = torch.max(clipwise_outputs, dim = 1)            \n            n_correct += torch.sum(preds == targets)            \n            \n#             optimizer.zero_grad()\n            loss.backward()            \n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n    \n            if batch_index == 67:\n                break\n\n        epoch_loss = np.mean(batch_losses_train)\n            \n            \n        # Evaluate\n        model.eval()\n        \n        batch_losses_val = []\n        n_correct_val = 0\n        \n        with torch.no_grad():\n            for batch_index, batch_data in enumerate(valid_loader, 0):\n                \n                for key in batch_data.keys():\n                    batch_data[key] = move_data_to_device(batch_data[key], device)\n                \n                waveforms = batch_data[\"waveform\"]\n                targets = batch_data[\"target\"]\n                \n                random_seed()\n                val_outputs = model(waveforms)\n                val_clipwise_outputs = val_outputs[\"clipwise_output\"]\n                \n                loss = loss_fn(val_clipwise_outputs, targets)\n#                 scheduler.step(loss)\n                batch_losses_val.append(loss.item())\n                _, val_preds = torch.max(val_clipwise_outputs, dim = 1)\n                n_correct_val += torch.sum(val_preds == targets) \n                \n#                 if batch_index == 6:\n#                     break\n                \n        epoch_loss_val = np.mean(batch_losses_val)\n        \n        if epoch == epochs - 1:\n            # Store val_loss of last epoch to get final averaged loss per run???\n            # run loss\n            validation_loss += epoch_loss_val\n            \n        \n                \n        dt = time.time() - start_time\n        lr = optimizer.param_groups[0]['lr']\n        \n        print(f'Epoch {epoch + 1}/{epochs} \\t lr={lr:.1e} \\t t={dt:.0f}s \\t loss={epoch_loss:.4f}, acc={n_correct.double() / n_samples_train:.4f} \\t val_loss={epoch_loss_val:.4f}, val_acc={n_correct_val.double() / n_samples_val:.4f}')\n\n        if epoch_loss_val <= validation_loss_min:\n            print(f'Validation loss decreased. Saving model... ')\n            torch.save(model.state_dict(), f'model_{fold}.pt')\n            validation_loss_min = epoch_loss_val\n        \n    # Predict on test set \n    batch_outputs_test = []\n    \n    with torch.no_grad():\n        for batch_index, batch_data in enumerate(test_loader, 0):\n            for key in batch_data.keys():\n                batch_data[key] = move_data_to_device(batch_data[key], device)\n                \n            waveforms = batch_data[\"waveform\"]\n            \n            random_seed()\n            test_outputs = model(waveforms)\n            test_outputs = F.softmax(test_outputs[\"clipwise_output\"], dim = 1)\n            test_outputs = test_outputs.cpu().detach().numpy()\n            batch_outputs_test.append(test_outputs)\n    \n    return validation_loss, np.vstack(batch_outputs_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_func(output_dict, target_dict):\n#     loss = - torch.mean(target_dict['target'] * output_dict['clipwise_output'])\n    # not dict\n    loss = - torch.mean(target_dict * output_dict)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nvalidation_losses_per_fold = []\ntest_predicitons_per_fold = []  \n\n# Folds\nfor fold in range(1, 4):\n    train_sampler = TrainSampler1(\n        hdf5_path=\"./features/waveform_train.h5\", \n        holdout_fold = fold, \n        batch_size = BATCH_SIZE * 2 if MIXUP else BATCH_SIZE\n    )\n\n    valid_sampler = EvaluateSampler(\n        hdf5_path = \"./features/waveform_train.h5\", \n        holdout_fold = fold, \n        batch_size = BATCH_SIZE\n    )\n    \n    test_sampler = TestSampler(\n        hdf5_path = \"./features/waveform_test.h5\", \n        batch_size = BATCH_SIZE\n    )   \n    \n    train_dataset = AudioDataset()\n    test_dataset = AudioDataset(train = False)\n\n    # Data loader\n    train_loader = DataLoader(dataset = train_dataset, \n        batch_sampler=train_sampler, collate_fn=collate_fn, \n        num_workers=2, pin_memory=True\n    )\n\n    valid_loader = DataLoader(dataset = train_dataset, \n        batch_sampler=valid_sampler, collate_fn=collate_fn, \n        num_workers=2, pin_memory=True\n    )    \n    \n    test_loader = DataLoader(dataset = test_dataset, \n        batch_sampler=test_sampler, collate_fn=collate_fn, \n        num_workers=2, pin_memory=True\n    )  \n    \n#     if 'mixup' in AUGMENTATION:\n#         MIXUP_AUGMENTER = Mixup(mixup_alpha = 1.)\n\n    random_seed()\n    Model = eval(MODEL_TYPE)\n    model = Model(SAMPLE_RATE, WINDOW_SIZE, HOP_SIZE, MEL_BINS, FMIN, FMAX, NUM_CLASSES, FREEZE_BASE)\n    model.load_from_pretrain(PRETRAINED_CHECKPOINT_PATH)\n    model.to(DEVICE)\n    \n    loss_fn = nn.CrossEntropyLoss().to(DEVICE)\n\n    print(f'Fold {fold}')\n\n    # Train, evaluate, predict\n    validation_loss, test_prediction = train_evaluate(model, train_loader = train_loader, valid_loader = valid_loader, test_loader = test_loader, loss_fn = loss_fn, lr = 1e-3, epochs = 30, warm_up_prop = 0.1, device = DEVICE, n_samples_train = 4339, n_samples_val = 370, fold = fold)\n    # Obtain validation result per fold\n    validation_losses_per_fold.append(validation_loss)\n    # Obtain test predictions per fold\n    test_predicitons_per_fold.append(test_prediction)\n\n    \n\nprint(\"=\" * 100)\n# Print summary validation result of all runs\n\nprint(f'Total avg val_loss={np.mean(validation_losses_per_fold)}, S/Dev={np.std(validation_losses_per_fold)}')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Total avg val_loss=1.5729644298553467, S/Dev=0.05230955180110382\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 20/20 \t lr=6.7e-09 \t t=26s \t loss=0.9724, acc=0.7412 \t val_loss=1.6636, val_acc=0.6189","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tp = np.mean(test_predicitons_per_fold, axis = 0)\n# tp = test_predicitons_per_fold[0]\ntp = pd.DataFrame(tp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = pd.read_csv(\"../input/giz-data/giz_data/SampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tp.columns = df_train_new[[\"label\", \"target\"]].sort_values(\"target\").drop_duplicates().label.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.iloc[:,1:] = tp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.iloc[:,1:].sum(axis = 1).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.iloc[:,1:].max(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.to_csv(\"cnn14_1.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?optim.lr_scheduler.OneCycleLR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n\n\ndef calculate_accuracy(y_true, y_score):\n    N = y_true.shape[0]\n    accuracy = np.sum(np.argmax(y_true, axis=-1) == np.argmax(y_score, axis=-1)) / N\n    return accuracy\n\n\nclass Evaluator(object):\n    def __init__(self, model):\n        self.model = model\n\n    def evaluate(self, data_loader):\n\n        # Forward\n        output_dict = forward(\n            model=self.model, \n            generator=data_loader, \n            return_target=True)\n\n        clipwise_output = output_dict['clipwise_output']    # (audios_num, classes_num)\n        target = output_dict['target']    # (audios_num, classes_num)\n\n        cm = metrics.confusion_matrix(np.argmax(target, axis=-1), np.argmax(clipwise_output, axis=-1), labels=None)\n        accuracy = calculate_accuracy(target, clipwise_output)\n\n        statistics = {'accuracy': accuracy}\n\n        return statistics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = Evaluator(model=model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?librosa.feature.melspectrogram","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Architecture(s) and requirements/models\nif putting in separate module, it should have it's own dependencies import"},{"metadata":{"trusted":true},"cell_type":"code","source":"matplotlib==3.0.3\nsoundfile==0.10.3.post1\nlibrosa==0.6.3\ntorch==1.0.1.post2\ntorchlibrosa==0.0.4\n\npip install -r requirements.txt+\n\nothers\n#librosa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchlibrosa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchlibrosa.stft import Spectrogram, LogmelFilterBank\nfrom torchlibrosa.augmentation import SpecAugmentation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_layer(layer):\n    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n    nn.init.xavier_uniform_(layer.weight)\n \n    if hasattr(layer, 'bias'):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_bn(bn):\n    \"\"\"Initialize a Batchnorm layer. \"\"\"\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        \n        super(ConvBlock, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=in_channels, \n                              out_channels=out_channels,\n                              kernel_size=(3, 3), stride=(1, 1),\n                              padding=(1, 1), bias=False)\n                              \n        self.conv2 = nn.Conv2d(in_channels=out_channels, \n                              out_channels=out_channels,\n                              kernel_size=(3, 3), stride=(1, 1),\n                              padding=(1, 1), bias=False)\n                              \n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.init_weight()\n        \n    def init_weight(self):\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n\n        \n    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n        \n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        x = F.relu_(self.bn2(self.conv2(x)))\n        if pool_type == 'max':\n            x = F.max_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg':\n            x = F.avg_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg+max':\n            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n            x2 = F.max_pool2d(x, kernel_size=pool_size)\n            x = x1 + x2\n        else:\n            raise Exception('Incorrect argument!')\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttBlock(nn.Module):\n    def __init__(self, n_in, n_out, activation='linear', temperature=1.):\n        super(AttBlock, self).__init__()\n        \n        self.activation = activation\n        self.temperature = temperature\n        self.att = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n        self.cla = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n        \n        self.bn_att = nn.BatchNorm1d(n_out)\n        self.init_weights()\n        \n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n        init_bn(self.bn_att)\n         \n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == 'linear':\n            return x\n        elif self.activation == 'sigmoid':\n            return torch.sigmoid(x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Cnn14(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n        fmax, classes_num):\n        \n        super(Cnn14, self).__init__()\n\n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n            freq_drop_width=8, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(64)\n\n        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n\n        self.fc1 = nn.Linear(2048, 2048, bias=True)\n        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n        \n        self.init_weight()\n\n    def init_weight(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        init_layer(self.fc_audioset)\n \n    def forward(self, input, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n\n        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n        \n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n        \n        if self.training:\n            x = self.spec_augmenter(x)\n\n        # Mixup on spectrogram\n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n\n        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = torch.mean(x, dim=3)\n        \n        (x1, _) = torch.max(x, dim=2)\n        x2 = torch.mean(x, dim=2)\n        x = x1 + x2\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu_(self.fc1(x))\n        embedding = F.dropout(x, p=0.5, training=self.training)\n        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n        \n        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n\n        return output_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Transfer_Cnn14(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n        fmax, classes_num, freeze_base):\n        \"\"\"Classifier for a new task using pretrained Cnn14 as a sub module.\n        \"\"\"\n        super(Transfer_Cnn14, self).__init__()\n        audioset_classes_num = 527\n        \n        self.base = Cnn14(sample_rate, window_size, hop_size, mel_bins, fmin, \n            fmax, audioset_classes_num)\n\n        # Transfer to another task layer\n        self.fc_transfer = nn.Linear(2048, classes_num, bias=True)\n\n        if freeze_base:\n            # Freeze AudioSet pretrained layers\n            for param in self.base.parameters():\n                param.requires_grad = False\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.fc_transfer)\n\n    def load_from_pretrain(self, pretrained_checkpoint_path):\n        checkpoint = torch.load(pretrained_checkpoint_path)\n        self.base.load_state_dict(checkpoint['model'])\n\n    def forward(self, input, mixup_lambda=None):\n        \"\"\"Input: (batch_size, data_length)\n        \"\"\"\n        output_dict = self.base(input, mixup_lambda)\n        embedding = output_dict['embedding']\n\n        clipwise_output =  torch.log_softmax(self.fc_transfer(embedding), dim=-1)\n        output_dict['clipwise_output'] = clipwise_output\n \n        return output_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}