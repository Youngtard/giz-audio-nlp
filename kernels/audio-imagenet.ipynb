{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Recommended - Run on kaggle kernel as most used packages come ready"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install resnest --pre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Requirements\n!pip install audiomentations[extras] --quiet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet_pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pretrainedmodels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchlibrosa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import confusion_matrix\n\nimport cv2\nimport IPython.display as ipd\nimport librosa\nimport pydub\nimport soundfile\nimport audiomentations\nimport albumentations\nfrom albumentations.pytorch import ToTensor\n\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets, models, transforms, utils\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n\nimport os\nimport warnings\nimport time\nimport copy\nimport random\n\nfrom torchlibrosa.augmentation import SpecAugmentation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !apt-get install ffmpeg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_seed(SEED = SEED):    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# update pytorch, torchvision, PIL/pillow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import PIL\nPIL.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ntorch.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train = pd.read_csv(\"data/Train.csv\")\n# sample_submission = pd.read_csv(\"data/SampleSubmission.csv\")\n# df_test = sample_submission[[\"fn\"]].copy()\n\ndf_train = pd.read_csv(\"../input/giz-data/giz_data/Train.csv\")\nsample_submission = pd.read_csv(\"../input/giz-data/giz_data/SampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ORIGINAL_AUDIO_PATH = \"../input/giz-data/giz_data/audio_files/\"\n# Named AdditionalUtterances\nADDITIONAL_AUDIO_PATH_1 = \"../input/giz-data/AdditionalUtterancesConverted/AdditionalUtterancesConverted/\"\nADDITIONAL_AUDIO_PATH_2 = \"../input/giz-data/AdditionalUtterancesConverted_2/AdditionalUtterancesConverted_2/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# match fnames in dataframes to audio path\ndf_train[\"fn\"] = df_train.fn.str.replace(\"audio_files/\", ORIGINAL_AUDIO_PATH)\nsample_submission[\"fn\"] = sample_submission.fn.str.replace(\"audio_files/\", ORIGINAL_AUDIO_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = sample_submission[[\"fn\"]].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"fold\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sss = StratifiedShuffleSplit(n_splits = 5, random_state = SEED, test_size = 0.2)\nkfold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = SEED)\n\nfor fold, (train_idx, valid_idx) in enumerate(kfold.split(df_train, df_train.label.values)):\n    \n#     df_train.loc[valid_idx, \"fold\"] = df_train.loc[valid_idx, \"fold\"].apply(lambda row: fold + 1 if row == -1 else row)\n    df_train.loc[valid_idx, \"fold\"] = fold + 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.fold.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Porblems with additional audio data like similar voicings/duplicates, sr was initially different, file type was initially mp3\ndf_train[\"weight\"] = 0.65","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add additional audio details to existing dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path_1 = ADDITIONAL_AUDIO_PATH_1 + \"latest_keywords\"\nadditional_labels_1 = os.listdir(base_path_1)\n\nbase_path_2 = ADDITIONAL_AUDIO_PATH_2 + \"nlp_keywords\"\nadditional_labels_2 = os.listdir(base_path_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_fns = []\nnew_labels = []\n# batch = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for label in additional_labels_1:\n    samples_in_label = os.listdir(os.path.join(base_path_1, label))\n    \n    for sample in samples_in_label:\n        file_path = os.path.join(base_path_1, label + \"/\" + sample)\n        new_fns.append(file_path)\n        new_labels.append(label)\n#         batch.append(1)\n\n        \nfor label in additional_labels_2:\n    samples_in_label = os.listdir(os.path.join(base_path_2, label))\n    \n    for sample in samples_in_label:\n        file_path = os.path.join(base_path_2, label + \"/\" + sample)\n        new_fns.append(file_path)\n        new_labels.append(label) \n#         batch.append(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = np.asarray([new_fns, new_labels]).T\n# df_new_ = np.reshape(df_new_, (-1, 2))\ndf_new = pd.DataFrame(df_new, columns = [\"fn\", \"label\"])\n\n# df_new[\"batch\"] = batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_new[\"id_\"] = df_new.fn.apply(lambda row: row.split(\"/\")[-1].split(\"_\")[0])\n# to_keep = df_new[[\"label\", \"id_\"]].drop_duplicates().index\n# df_new = df_new.iloc[to_keep].reset_index(drop = True)\n# df_new = df_new.drop(\"id_\", axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_new[\"id_\"] = df_new.fn.apply(lambda row: row.split(\"/\")[-1].split(\"_\")[0])\n# to_keep = df_new[df_new.batch == 1][[\"label\", \"id_\"]].drop_duplicates().index.append(df_new[df_new.batch == 2].index)\n# df_new = df_new.iloc[to_keep].reset_index(drop = True)\n# df_new = df_new.drop(\"id_\", axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use all for training - identified by -1\ndf_new[\"fold\"] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new[\"weight\"] = 0.35","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New df to be used for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concat\ndf_train_new = pd.concat([df_train, df_new], axis = 0).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort\ndf_train_new = df_train_new.sort_values(\"label\").reset_index(drop = True)\n# Shuffle\ndf_train_new = df_train_new.sample(frac=1, random_state = 42).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train_new = df_train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_new.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample an audio"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = '../input/giz-data/giz_data/audio_files/00118N3.wav'\nipd.Audio(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = '../input/giz-data/AdditionalUtterancesConverted/AdditionalUtterancesConverted/latest_keywords/abalimi/5aa4893526794a73a3ed0a7ebba2a7bf_4b41e8fd3c364e5082d1b2b5bb64c8d9.wav'\ny, sr = librosa.load(sample, sr = None, mono = True)\nyt, index = librosa.effects.trim(y)\nipd.Audio(yt, rate = 22050)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get max number of samples i.e len(y) which signifies longest sound","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ipd.Audio(np.pad(y, (0, 9999)), rate = 22050)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y), len(y) / 22050, librosa.get_duration(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(np.pad(y, (0, 1)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick Label encoding\ndf_train_new[\"target\"] = df_train_new.label.astype(\"category\").cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"target\"] = df_train.label.astype(\"category\").cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Util functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://github.com/TheoViel/kaggle_birdcall_identification/blob/master/src/data/transforms.py\ndef mono_to_color(X, eps=1e-6, mean=None, std=None):\n    \"\"\"\n    Converts a one channel array to a 3 channel one in [0, 255]\n    Arguments:\n        X {numpy array [H x W]} -- 2D array to convert\n    Keyword Arguments:\n        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n        mean {None or np array} -- Mean for normalization (default: {None})\n        std {None or np array} -- Std for normalization (default: {None})\n    Returns:\n        numpy array [3 x H x W] -- RGB numpy array\n    \"\"\"\n    X = np.stack([X, X, X], axis=-1)\n\n#     # Standardize\n#     mean = mean or X.mean()\n#     std = std or X.std()\n#     X = (X - mean) / (std + eps)\n\n    # Normalize to [0, 255]\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\ndef normalize(image):\n    MEAN = np.array([0.485, 0.456, 0.406])\n    STD = np.array([0.229, 0.224, 0.225])\n    \n    image = (image / 255.0).astype(np.float32)\n    image = (image - MEAN) / STD\n    return np.moveaxis(image, 2, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://github.com/TheoViel/kaggle_birdcall_identification/blob/master/src/data/transforms.py\ndef mono_to_color(power, pcen, amp, eps=1e-6, mean=None, std=None):\n    \"\"\"\n    Converts a one channel array to a 3 channel one in [0, 255]\n    Arguments:\n        X {numpy array [H x W]} -- 2D array to convert\n    Keyword Arguments:\n        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n        mean {None or np array} -- Mean for normalization (default: {None})\n        std {None or np array} -- Std for normalization (default: {None})\n    Returns:\n        numpy array [3 x H x W] -- RGB numpy array\n    \"\"\"\n    X = np.stack([power, pcen, amp], axis=-1)\n\n#     # Standardize\n#     mean = mean or X.mean()\n#     std = std or X.std()\n#     X = (X - mean) / (std + eps)\n\n    # Normalize to [0, 255]\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) / (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\ndef normalize(image):\n    MEAN = np.array([0.485, 0.456, 0.406])\n    STD = np.array([0.229, 0.224, 0.225])\n    \n    image = (image / 255.0).astype(np.float32)\n    image = (image - MEAN) / STD\n    return np.moveaxis(image, 2, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spec_to_image(spec, eps=1e-6):\n  mean = spec.mean()\n  std = spec.std()\n  spec_norm = (spec - mean) / (std + eps)\n  spec_min, spec_max = spec_norm.min(), spec_norm.max()\n  spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n  spec_scaled = spec_scaled.astype(np.uint8)\n  return spec_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n#     # Stack X as [X,X,X]\n#     X = np.stack([X, X, X], axis=-1)\n\n#     # Standardize\n#     mean = mean or X.mean()\n#     X = X - mean\n#     std = std or X.std()\n#     Xstd = X / (std + eps)\n#     _min, _max = Xstd.min(), Xstd.max()\n#     norm_max = norm_max or _max\n#     norm_min = norm_min or _min\n#     if (_max - _min) > eps:\n#         # Normalize to [0, 255]\n#         V = Xstd\n#         V[V < norm_min] = norm_min\n#         V[V > norm_max] = norm_max\n#         V = 255 * (V - norm_min) / (norm_max - norm_min)\n#         V = V.astype(np.uint8)\n#     else:\n#         # Just zero\n#         V = np.zeros_like(Xstd, dtype=np.uint8)\n#     return V","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_audio_transforms():\n    random_seed()\n    transforms = audiomentations.Compose(\n        [\n#             audiomentations.FrequencyMask(p = 0.5),\n#             audiomentations.AddGaussianSNR(max_SNR = 0.5),\n#             audiomentations.AddGaussianNoise(),\n#             audiomentations.TimeStretch(p = 0.5, leave_length_unchanged = False),\n#             audiomentations.PitchShift(p = 0.5),\n#             audiomentations.Shift(p = 0.5),\n            # Normalize sound levels\n            audiomentations.Normalize(p = 1.0),\n#             audiomentations.Trim(),\n#             audiomentations.ClippingDistortion(),\n# #             audiomentations.AddBackgroundNoise(sounds_path=BACKGROUND_PATH, min_snr_in_db=0, max_snr_in_db=2, p=0.5)\n#             audiomentations.Gain(),\n        ]\n    )\n    random_seed()\n    return transforms\n\ndef get_audio_transforms2():\n    random_seed()\n    transforms = audiomentations.Compose(\n        [\n#             audiomentations.FrequencyMask(p = 0.5),\n#             audiomentations.AddGaussianSNR(max_SNR = 0.25),\n#             audiomentations.AddGaussianNoise(),\n#             audiomentations.TimeStretch(leave_length_unchanged = False),\n#             audiomentations.PitchShift(),\n#             audiomentations.Shift(),\n            # Normalize sound levels\n            audiomentations.Normalize(p = 1.0),\n#             audiomentations.Trim(),\n#             audiomentations.ClippingDistortion(),\n# #             audiomentations.AddBackgroundNoise(sounds_path=BACKGROUND_PATH, min_snr_in_db=0, max_snr_in_db=2, p=0.5)\n#             audiomentations.Gain(),\n        ]\n    )\n    random_seed()\n    return transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_image_transforms(train = True):\n    \n    needed_transforms =         [\n            albumentations.Resize(224, 224),\n            albumentations.Normalize(),\n            ToTensor(),\n    \n    transforms = albumentations.Compose(\n        [\n            albumentations.Resize(224, 224),\n            albumentations.Normalize(),\n            ToTensor(),\n            if train:\n                albumentations.HorizontalFlip(),\n#             albumentations.RandomBrightness(),\n        #     albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n        #     albumentations.JpegCompression(80),\n#             albumentations.HueSaturationValue(), \n            else:\n                pass\n        ]\n    )\n        \n    return transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mel_spect = librosa.feature.melspectrogram(wav, sr = sr, n_mels = 128, fmin = 20, fmax = sr / 2.0,)\nmel_spect = librosa.power_to_db(mel_spect)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 channel\nmel_spect.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3 channels \nmono_to_color(mel_spect).shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"?torchaudio.transforms.FrequencyMasking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mask_transforms():    \n    mask_transforms = nn.Sequential(\n#         torchaudio.transforms.MelSpectrogram(sample_rate=22050, n_mels=128),\n        torchaudio.transforms.FrequencyMasking(freq_mask_param = 16),\n        torchaudio.transforms.TimeMasking(time_mask_param = 32)\n#         torchaudio.transforms.TimeStretch(p)\n#         torchaudio.transforms.FrequencyMasking(),\n#         torchaudio.transforms.TimeMasking()\n    )\n    \n    return mask_transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ?librosa.feature.melspectrogram\n?librosa.amplitude_to_db","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SAMPLING_RATE = 22050","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yy, srr = librosa.load(df_train_new.fn[1234], sr = None, mono = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ipd.Audio(yy, rate = 22050)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_new.fn[123]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_new.fn[99]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(yy) / 22050","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MelspectrogramData(Dataset):\n    def __init__(self, df, audio_transforms = None, image_transforms = None, mask_transforms = None, train = True, visualization = False, sr = None):\n        self.df = df  \n        self.audio_files = df.fn.values\n        # Train time audio and image augmentation\n        self.audio_transforms = audio_transforms\n        self.image_transforms = image_transforms \n        self.mask_transforms = mask_transforms\n        self.train = train\n        self.visualization = visualization\n        self.sr = sr\n        \n        if train:\n            self.targets = self.df.target\n            self.weights = self.df.weight\n        if visualization:\n            self.labels = self.df.label\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def get_mel_spect(self, y, sr):\n        mel_spect = librosa.feature.melspectrogram(\n            y, sr = sr, n_mels = 128, fmin = 20, fmax = sr // 2.0, n_fft = 2048, hop_length = 512,\n        )\n\n        mel_spect = librosa.power_to_db(mel_spect)\n#         mel_spect = librosa.amplitude_to_db(mel_spect)\n        \n        return mel_spect\n\n    def __getitem__(self, idx):\n        # y == wav filfe\n        sr = self.sr\n        train = self.train\n        visualization = self.visualization\n        \n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UserWarning)\n\n            y, sr = librosa.load(self.audio_files[idx], sr = self.sr, mono = True, res_type = \"kaiser_fast\")\n#             y, sr = librosa.core.audio.load(self.audio_files[idx])\n#             y = librosa.to_mono(y)\n#             if sr != SAMPLING_RATE:\n#                 y = librosa.resample(y, sr, SAMPLING_RATE, res_type = 'kaiser_fast')\n#             y, _ = librosa.effects.trim(y)\n    #         y, sr = soundfile.read(self.audio_files[idx])\n    #         print(sr)\n\n        if self.audio_transforms != None:\n            # Apply audio augmentations (to train)\n            y = self.audio_transforms(samples = y, sample_rate = sr)\n\n        mel_spect = self.get_mel_spect(y, sr)\n        \n        if self.mask_transforms != None:\n            # Applied to mel (spect)\n            mel_spect = self.mask_transforms(torch.tensor(mel_spect))\n\n        img = mono_to_color(mel_spect)\n#         img = spec_to_image(mel_spect)\n#         img = normalize(img)\n        \n    \n            \n#         img = spec_to_image(mel_spect)\n        # Apply image augmentations\n        # specific train and test image transforms is specified later\n#         img = Image.fromarray(img)\n#         img = self.image_transforms(img)\n        img = self.image_transforms(image = img)\n#         img = self.image_transforms(image = mel_spect)\n        \n        if self.train:\n        # Ensure no trnTAudioAugment. is applied to validation \n            if self.visualization:\n                label = self.labels[idx]\n                return {\n                    \"img\": img,\n                    \"label\": label                   \n                }\n            else:\n                #Convert to tensor because needed for modelling operations using tensors\n                target = torch.tensor(self.targets[idx], dtype = torch.long)\n                weight = torch.tensor(self.weights[idx], dtype = torch.float16)\n                return {\n                    \"img\": img,\n                    \"target\": target,\n                    \"weight\": weight\n                }\n        else:\n            return {\n                \"img\": img\n            }\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?SpecAugmentation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MelspectrogramData(Dataset):\n    def __init__(self, df, audio_transforms = None, image_transforms = None, mask_transforms = None, train = True, visualization = False, sr = None):\n        self.df = df  \n        self.audio_files = df.fn.values\n        # Train time audio and image augmentation\n        self.audio_transforms = audio_transforms\n        self.image_transforms = image_transforms \n        self.mask_transforms = mask_transforms\n        self.train = train\n        self.visualization = visualization\n        self.sr = sr\n#         self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, freq_drop_width=8, freq_stripes_num=2)\n        \n        if train:\n            self.targets = self.df.target\n            self.weights = self.df.weight\n        if visualization:\n            self.labels = self.df.label\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def get_mel_spect(self, y, sr):\n        mel_spect = librosa.feature.melspectrogram(\n            y, sr = sr, n_mels = 128, fmin = 20, fmax = sr // 2.0, n_fft = 2048, hop_length = 512,\n        )\n\n#         mel_spect = librosa.power_to_db(mel_spect)\n#         mel_spect = librosa.amplitude_to_db(mel_spect)\n        \n        return mel_spect\n\n    def __getitem__(self, idx):\n        # y == wav filfe\n        sr = self.sr\n        train = self.train\n        visualization = self.visualization\n        \n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UserWarning)\n\n            y, sr = librosa.load(self.audio_files[idx], sr = self.sr, mono = True, res_type = \"kaiser_fast\")\n#             y, sr = librosa.core.audio.load(self.audio_files[idx])\n#             y = librosa.to_mono(y)\n#             if sr != SAMPLING_RATE:\n#                 y = librosa.resample(y, sr, SAMPLING_RATE, res_type = 'kaiser_fast')\n#             y, _ = librosa.effects.trim(y)\n    #         y, sr = soundfile.read(self.audio_files[idx])\n    #         print(sr)\n\n        if self.audio_transforms != None:\n            # Apply audio augmentations (to train)\n            y = self.audio_transforms(samples = y, sample_rate = sr)\n\n        mel_spect = self.get_mel_spect(y, sr)\n        \n        power = librosa.power_to_db(mel_spect)\n        pcen = librosa.pcen(mel_spect, sr = sr)\n        amp = librosa.amplitude_to_db(mel_spect)\n        \n#         p = self.spec_augmenter(torch.tensor(power))\n        \n        if self.mask_transforms != None:\n            # Applied to mel (spect)\n#             mel_spect = self.mask_transforms(torch.tensor(mel_spect))\n            power = self.mask_transforms(torch.tensor(power))\n            pcen = self.mask_transforms(torch.tensor(pcen))\n            amp = self.mask_transforms(torch.tensor(amp))           \n\n        img = mono_to_color(power, pcen, amp)\n#         img = spec_to_image(mel_spect)\n#         img = normalize(img)\n        \n    \n            \n#         img = spec_to_image(mel_spect)\n        # Apply image augmentations\n        # specific train and test image transforms is specified later\n#         img = Image.fromarray(img)\n#         img = self.image_transforms(img)\n        img = self.image_transforms(image = img)\n#         img = self.image_transforms(image = mel_spect)\n        \n        if self.train:\n        # Ensure no trnTAudioAugment. is applied to validation \n            if self.visualization:\n                label = self.labels[idx]\n                return {\n                    \"img\": img,\n                    \"label\": label                   \n                }\n            else:\n                #Convert to tensor because needed for modelling operations using tensors\n                target = torch.tensor(self.targets[idx], dtype = torch.long)\n                weight = torch.tensor(self.weights[idx], dtype = torch.float16)\n                return {\n                    \"img\": img,\n                    \"target\": target,\n                    \"weight\": weight\n                }\n        else:\n            return {\n                \"img\": img\n            }\n\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vizualize some data"},{"metadata":{"trusted":true},"cell_type":"code","source":"?albumentations.Resize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_data(target_to_sample = \"any\"):\n    assert target_to_sample == \"any\" or type(target_to_sample) == int\n    \n    if target_to_sample == \"any\":\n        # Use all train data\n        sample_df = df_train_new\n    else:\n        sample_df = df_train_new[df_train_new.target == target_to_sample].reset_index(drop = True)\n        \n    image_transforms = albumentations.Compose([\n        albumentations.Resize(224, 224),\n    #     albumentations.HorizontalFlip(),\n    #     albumentations.RandomBrightness(),\n    #     albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n    #     albumentations.JpegCompression(80),\n    #     albumentations.HueSaturationValue(),\n#         albumentations.Normalize(mean=(0.5), std=(0.2)),\n        albumentations.Normalize(),\n        ToTensor()\n    ])        \n    \n    sample_data = MelspectrogramData(sample_df, get_audio_transforms(), image_transforms, mask_transforms = get_mask_transforms(), visualization = True)\n    sample_loader = DataLoader(sample_data, batch_size = 8, shuffle = True, num_workers = 0)\n    \n    sample_data_iter = iter(sample_loader)\n    sample_output = sample_data_iter.next()\n    sample_images = sample_output[\"img\"][\"image\"].numpy()\n    sample_labels = sample_output[\"label\"]\n    \n    fig = plt.figure(figsize = (24, 8))\n\n    for idx in np.arange(len(sample_images)):\n        ax = fig.add_subplot(2, 8/2, idx + 1, xticks = [], yticks = [])\n        ax.imshow(np.squeeze(sample_images[idx]).transpose(1, 2, 0), cmap = \"gray\")\n#         ax.imshow(np.squeeze(sample_images[idx]), cmap = \"gray\")\n        ax.set_title(str(sample_labels[idx]))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# warnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_data(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_data(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_transforms_train = albumentations.Compose([\n    albumentations.Resize(224, 224),\n#     albumentations.HorizontalFlip(),\n#     albumentations.RandomBrightness(),\n#     albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n#     albumentations.JpegCompression(80),\n#     albumentations.HueSaturationValue(),\n    albumentations.Normalize(),\n    ToTensor()\n    ])\nimage_transforms_test = albumentations.Compose([\n    albumentations.Resize(512, 512),\n    albumentations.Normalize(),\n    ToTensor()\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use language as a feature, number of syllables, EDA\n# df_train.label.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# device = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet\nimport pretrainedmodels\nfrom resnest.torch import resnest50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn.Sequential(\n    nn.Dropout(0.3),\n    nn.Linear(in_features, 193)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(p.numel() for p in get_model().parameters() if p.requires_grad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 1\nfor child in get_model().children():\n    print(i)\n    print(child)\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# put model into a model class?\n# pass model name arg to get_model() function","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrainedmodels.model_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?models.resnext50_32x4d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n#     random_seed()\n    model = models.resnext50_32x4d(pretrained=True)\n#     model = models.resnet50(pretrained=True)\n#     model = models.mobilenet_v2(pretrained = True)\n#     model = models.wide_resnet50_2(pretrained = True)\n#     model = resnest50(pretrained = True)\n#     pretrainedmodels.se\n\n#     print(model.fc.in_features)\n    random_seed()\n    in_features = model.fc.in_features\n    #vgg\n#     in_features = model.classifier[6].in_features\n    # mobilenet\n#     in_features = model.classifier[1].in_features\n\n    random_seed()\n    for p in model.parameters():\n        random_seed()\n        p.requires_grad = True\n     \n#     cc = 1\n#     for child in model.children():    \n#         if cc <= 6:\n#             for param in child.parameters():\n#                 param.requires_grad = False\n#         cc += 1\n\n#     random_seed()\n#     model.fc = nn.Linear(in_features, 193)\n#     nn.init.xavier_normal_(model.fc.weight)\n#     torch.manual_seed(SEED)\n#     model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n#     model.conv1.weight = nn.Parameter(model.conv1.weight.sum(dim=1, keepdim=True))\n\n#     model.avgpool = nn.AdaptiveMaxPool2d(output_size = (1, 1))\n    model.fc = nn.Sequential(\n#         nn.Dropout(0.5),\n        nn.Linear(in_features, 193)\n    )\n    \n#     model.classifier = nn.Sequential(\n#         nn.Dropout(p=0.5, inplace = False),\n#         nn.Linear(in_features, out_features = 193, bias=True)\n#     ) \n\n#     model.classifier[6].out_features = 193\n    \n    \n    \n   \n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNext(nn.Module):\n    def __init__(self, base_model_name: str, pretrained=True,\n                 num_classes=193):\n        super().__init__()\n        base_model = models.__getattribute__(base_model_name)(\n            pretrained=pretrained)\n        layers = list(base_model.children())[:-1]\n#         layers = list(base_model.children())\n#         layers.append(nn.AdaptiveMaxPool2d((1, 1)))\n        self.encoder = nn.Sequential(*layers)\n\n        in_features = base_model.fc.in_features\n\n        self.classifier = nn.Sequential(\n#             nn.Linear(in_features, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n#             nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(p=0.2),\n#             nn.Linear(1024, num_classes)\n            nn.Linear(in_features, num_classes)\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n#         print(x.shape)\n#         x = torch.mean(x, dim=1)\n#         print(x.shape)\n        x = self.encoder(x).view(batch_size, -1)\n#         print(x.shape)\n        x = self.classifier(x)\n#         multiclass_proba = F.softmax(x, dim=1)\n#         multilabel_proba = F.sigmoid(x)\n#         return {\n#             \"logits\": x,\n#             \"multiclass_proba\": multiclass_proba,\n#             \"multilabel_proba\": multilabel_proba\n#         }\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.tensor([[1,2]]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ResNext(\"resnext50_32x4d\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet34\nresnext50_32x4d\n\ndpn68b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(model_name):\n#     random_seed()\n    model = pretrainedmodels.__dict__[model_name](pretrained = \"imagenet+5k\")\n#     model = pretrainedmodels.__dict__[model_name](pretrained = \"imagenet\")\n#     model = models.resnet34(pretrained=True)\n\n#     print(model.fc.in_features)\n#     random_seed()\n    #dpn\n    in_channels = model.last_linear.in_channels\n    #dense\n#     in_features = model.last_linear.in_features\n\n#     random_seed()\n#     for p in model.parameters():\n#         random_seed()\n#         p.requires_grad = True\n        \n    random_seed()\n    for p in model.features[0:24].parameters():\n        random_seed()\n        #Freeze first 10 model.features layers\n        p.requires_grad = False     \n\n        \n#     for m in model.modules():\n#         if isinstance(m, nn.Conv2d):\n#             nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n#         elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n#             nn.init.constant_(m.weight, 1)\n#             nn.init.constant_(m.bias, 0)    \n        \n    # main_module \n#     for child in model.children():\n#         main_module = child\n#         i = 1\n#         for block in main_module:\n#             # Dual Path Blocks\n#             blocks.append(block)\n\n#             if i <= 20:\n#                 for param in block.parameters():\n#                     param.requires_grad = False\n                    \n#                 del block\n                \n#             i += 1\n            \n#         # 2nd child consisting of last Conv Layer is not needed\n#         break    \n        \n#     model.features = nn.Sequential(*list(model.features.children())[:-4])\n    \n#     cc = 1\n#     for child in model.children():    \n#         if cc <= 6:\n#             for param in child.parameters():\n#                 param.requires_grad = False\n#         cc += 1\n\n#     random_seed()\n#     model.fc = nn.Linear(in_features, 193)\n#     nn.init.xavier_normal_(model.fc.weight)\n#     torch.manual_seed(SEED)\n#     model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n#     model.conv1.weight = nn.Parameter(model.conv1.weight.sum(dim=1, keepdim=True))\n\n#     model.last_linear = nn.Sequential(\n# #         nn.Dropout(0.5),\n#         nn.Linear(in_features, 193)\n#     )\n    \n    #dpn\n#     model.last_linear = nn.Conv2d(in_channels, 193, kernel_size=(1, 1), stride=(1, 1))\n    model.last_linear = nn.Sequential(\n#         nn.Linear(in_features, out_features = 193, bias=True)\n#         nn.Dropout2d(0.2),\n        nn.Conv2d(in_channels, 193, kernel_size=(1, 1), stride=(2, 2)),\n#         nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#         nn.Linear(in_channels, out_features = 193, bias=True),\n#         nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n#         nn.Linear(512, out_features = 193, bias = True)\n    )    \n#     dense\n#     model.last_linear = nn.Linear(in_features, out_features = 193, bias=True)\n    \n    \n    \n   \n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model(\"dpn68b\").features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?librosa.feature.melspectrogram","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = pretrainedmodels.__dict__[\"dpn68b\"]()#='imagenet+5k'\n# m = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"]()\n# m = models.vgg11_bn()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.last_linear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# main_module \nblocks = []\nfor child in m.children():\n    main_module = child\n    i = 1\n    for block in main_module:\n        main_module.rem      \n        # Dual Path Blocks\n        blocks.append(block)\n        \n#         if i <= 24:\n#             for param in block.parameters():\n#                 param.requires_grad = True\n    # 2nd child consisting of last Conv Layer is not needed\n\n    break\nlen(blocks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blocks[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(nn.Sequential(*list(m.features.children())[:-4]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modules = []\nfor mi in m.modules():\n    \n#     print(\"**************************************\")\n    if isinstance(mi, nn.Conv2d):\n        modules.append(mi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(modules)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modules[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.last_linear.in_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m.last_linear.in_channels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model(\"se_resnet50\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n#     random_seed()\n    model = EfficientNet.from_name('efficientnet-b0')\n\n#     print(model.fc.in_features)\n#     random_seed()\n    in_features = model._fc.in_features\n\n#     random_seed()\n    for p in model.parameters():\n        random_seed()\n        p.requires_grad = True\n\n#     random_seed()\n    # model.fc = nn.Linear(in_features, 193)\n#     torch.manual_seed(SEED)\n    model._fc = nn.Sequential(\n#         nn.Dropout(0.5),\n        nn.Linear(in_features, 193)\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed()\nmodel = models.resnet34(pretrained=True)\n\nprint(model.fc.in_features)\nrandom_seed()\nin_features = model.fc.in_features\n\nrandom_seed()\nfor p in model.parameters():\n    random_seed()\n    p.requires_grad = True\n    \nrandom_seed()\n# model.fc = nn.Linear(in_features, 193)\ntorch.manual_seed(SEED)\nmodel.fc = nn.Sequential(\n    nn.Dropout(0.3),\n    nn.Linear(in_features, 193)\n)\n\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = models.resnet34(pretrained=True)\n# model.fc = nn.Linear(512,50)\n# model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 4), stride=(1, 1), padding=(3, 3), bias=False)\n# model = model.to(device)\n\nmodel = EfficientNet.from_name('efficientnet-b0')\nfor p in model.parameters():\n    p.requires_grad = True\nin_features = model._fc.in_features\nmodel._fc = nn.Linear(in_features, 193)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CustomNet()\n# print(model)\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def focal_loss(labels, logits, alpha, gamma):\n    \"\"\"Compute the focal loss between `logits` and the ground truth `labels`.\n    Focal loss = -alpha_t * (1-pt)^gamma * log(pt)\n    where pt is the probability of being classified to the true class.\n    pt = p (if true class), otherwise pt = 1 - p. p = sigmoid(logit).\n    Args:\n      labels: A float tensor of size [batch, num_classes].\n      logits: A float tensor of size [batch, num_classes].\n      alpha: A float tensor of size [batch_size]\n        specifying per-example weight for balanced cross entropy.\n      gamma: A float scalar modulating loss from hard and easy examples.\n    Returns:\n      focal_loss: A float32 scalar representing normalized total loss.\n    \"\"\"    \n    BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = \"none\")\n\n    if gamma == 0.0:\n        modulator = 1.0\n    else:\n        modulator = torch.exp(-gamma * labels * logits - gamma * torch.log(1 + \n            torch.exp(-1.0 * logits)))\n\n    loss = modulator * BCLoss\n\n    weighted_loss = alpha * loss\n    focal_loss = torch.sum(weighted_loss)\n\n    focal_loss /= torch.sum(labels)\n    return focal_loss\n\n\n\ndef CB_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma):\n    \"\"\"Compute the Class Balanced Loss between `logits` and the ground truth `labels`.\n    Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n    where Loss is one of the standard losses used for Neural Networks.\n    Args:\n      labels: A int tensor of size [batch].\n      logits: A float tensor of size [batch, no_of_classes].\n      samples_per_cls: A python list of size [no_of_classes].\n      no_of_classes: total number of classes. int\n      loss_type: string. One of \"sigmoid\", \"focal\", \"softmax\".\n      beta: float. Hyperparameter for Class balanced loss.\n      gamma: float. Hyperparameter for Focal loss.\n    Returns:\n      cb_loss: A float tensor representing class balanced loss\n    \"\"\"\n    effective_num = 1.0 - np.power(beta, samples_per_cls)\n    weights = (1.0 - beta) / np.array(effective_num)\n    weights = weights / np.sum(weights) * no_of_classes\n\n    labels_one_hot = F.one_hot(labels, no_of_classes).float()\n\n    weights = torch.tensor(weights).float()\n    weights = weights.unsqueeze(0)\n    weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot\n    weights = weights.sum(1)\n    weights = weights.unsqueeze(1)\n    weights = weights.repeat(1,no_of_classes)\n\n    if loss_type == \"focal\":\n        cb_loss = focal_loss(labels_one_hot, logits, weights, gamma)\n    elif loss_type == \"sigmoid\":\n        cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weights = weights)\n    elif loss_type == \"softmax\":\n        pred = logits.softmax(dim = 1)\n        cb_loss = F.binary_cross_entropy(input = pred, target = labels_one_hot, weight = weights)\n    return cb_loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F.one_hot(torch.tensor(df_train_new.target, dtype = torch.long), 193)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wd = MelspectrogramData(train_set, audio_transforms = get_audio_transforms(), image_transforms = train_transforms, mask_transforms = None, train = True, sr=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in DataLoader(wd, batch_size = 32, shuffle = False, num_workers = 0):\n    # 1st batch\n    dd = i\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = torch.zeros(193, dtype=torch.long)\ncls = []\nfor target in F.one_hot(dd[\"target\"], 193):\n    labels += target\n    cls.append(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(labels.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mixup_data(x, y, alpha=0.4):\n    \"\"\"\n    Applies mixup to a sample\n    Arguments:\n        x {torch tensor} -- Input batch\n        y {torch tensor} -- Labels\n    Keyword Arguments:\n        alpha {float} -- Parameter of the beta distribution (default: {0.4})\n    Returns:\n        torch tensor  -- Mixed input\n        torch tensor  -- Labels of the original batch\n        torch tensor  -- Labels of the shuffle batch\n        float  -- Probability samples by the beta distribution\n    \"\"\"\n    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n\n    index = torch.randperm(x.size()[0]).cuda()\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n\n    return mixed_x, y_a, y_b, lam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchtoolbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtoolbox.optimizer import Lookahead\nfrom torchtoolbox.tools import mixup_data, mixup_criterion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?mixup_criterion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_mixup(x, mixup_lambda):\n    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes \n    (1, 3, 5, ...).\n    Args:\n      x: (batch_size * 2, ...)\n      mixup_lambda: (batch_size * 2,)\n    Returns:\n      out: (batch_size, ...)\n    \"\"\"\n    out = (x[0 :: 2].transpose(0, -1) * mixup_lambda[0 :: 2] + \\\n        x[1 :: 2].transpose(0, -1) * mixup_lambda[1 :: 2]).transpose(0, -1)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_evaluate_predict(model, train_data, valid_data, test_data, loss_fn, lr, epochs, batch_size, warm_up_prop, device, n_samples_train, n_samples_val, fold):\n    \n    train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, num_workers = 2)\n    valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = False, num_workers = 2)\n    test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False, num_workers = 2)\n    \n    num_training_steps = epochs * len(train_loader)\n    num_warmup_steps = int(warm_up_prop * num_training_steps)\n    print(num_training_steps, num_warmup_steps)\n#     optimizer = optim.SGD(model.parameters(), lr = lr, momentum = 0.9, weight_decay = 0.01)\n#     optimizer = optim.RMSprop(model.parameters(), lr = lr)\n#     optimizer = optim.Adam(model.parameters(), lr = lr)\n    optimizer = optim.AdamW(model.parameters(), lr = lr, weight_decay = 0.1)\n#     optimizer = optim.AdamW([\n#         {\"params\": model.features.parameters()},\n#         {\"params\": model.last_linear.parameters(), \"lr\": 1e-2}\n#     ], lr = lr, weight_decay = 0.1)\n#     optim.SGD([\n# {‘params’: model.base.parameters()},\n# {‘params’: model.classifier.parameters(), ‘lr’: 1e-3}\n# ], lr=1e-2, momentum=0.9)\n#     optimizer = Lookahead(optimizer)\n#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps) #using transformers library\n#     scheduler = get_linear_schedule_with_warmup(optimizer, 0, 1) #using transformers library    \n#     scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps) #using transformers library\n#     scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-10, max_lr=1e-3)\n#     scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.01)\n#     scheduler = optim.lr_scheduler.StepLR(optimizer)\n#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5)\n#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=int(len(train_loader)), epochs=epochs)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps = num_training_steps)\n#     scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader))\n#     optimizer = Lookahead(optimizer)\n    \n    cm_targets = []\n    cm_preds = []\n\n    \n    validation_loss = 0\n    validation_loss_min = np.Inf\n    \n#     scheduler.step()\n    \n    for epoch in range(epochs):\n        # Train\n        model.train()\n        start_time = time.time()\n        \n        batch_losses_train = []\n        n_correct = 0\n        \n        for batch_index, data in enumerate(train_loader, 0):\n            \n            imgs = data[\"img\"][\"image\"].to(device)#.unsqueeze(1)\n#             imgs = data[\"img\"].to(device)#.unsqueeze(1)\n#             targets = data[\"target\"].to(device, dtype = torch.long).unsqueeze(1)\n            targets = data[\"target\"].to(device, dtype = torch.long)  \n            sample_weights = data[\"weight\"].to(device, dtype = torch.float16)\n        \n#             imgs = do_mixup(imgs, (batch_size * 2,))\n        \n#             imgs, labels_a, labels_b, lam = mixup_data(imgs, targets, 0.2)\n        \n# #             if np.random.rand() < mixup_proba:\n#             if np.random.rand() < 0.5:\n#                 imgs, y_a, y_b, _ = mixup_data(imgs, targets, alpha=5)\n#                 targets = torch.clamp(y_a + y_b, 0, 1)        \n\n            random_seed()\n            outputs = model(imgs)\n#             loss = loss_fn(outputs, targets.float())\n#             loss = loss_fn(outputs.logits, targets)\n            loss = loss_fn(outputs, targets)\n#             loss = mixup_criterion(loss_fn, outputs, labels_a, labels_b, lam)\n    \n            #=======\n#             samples_per_cls = torch.zeros(193, dtype=torch.long)\n#             samples_per_cls.to(device)\n#             for target in F.one_hot(targets, 193):\n#                 target.to(device)\n#                 samples_per_cls += target\n#             samples_per_cls = list(samples_per_cls.numpy())\n            \n#             beta = 0.9999\n#             gamma = 2.0\n            \n#             loss = CB_loss(targets, outputs, samples_per_cls, 193, \"focal\", beta, gamma)\n            \n            #==========\n#             loss = loss * sample_weights\n#             loss = (loss * sample_weights / sample_weights.sum()).sum()\n            \n            batch_losses_train.append(loss.item())\n#             _, preds = torch.max(outputs.logits, dim = 1)\n            _, preds = torch.max(outputs, dim = 1)            \n            n_correct += torch.sum(preds == targets)            \n            \n            loss.backward()            \n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n#         scheduler.step()\n\n        epoch_loss = np.mean(batch_losses_train)\n            \n            \n        # Evaluate\n        model.eval()\n        \n        batch_losses_val = []\n        n_correct_val = 0\n        \n#         cm_targets = []\n#         cm_preds = []\n        \n        with torch.no_grad():\n            for batch_index, data in enumerate(valid_loader, 0):\n                # n == batch_size\n                imgs = data[\"img\"][\"image\"].to(device)#.unsqueeze(1)\n#                 imgs = data[\"img\"].to(device)#.unsqueeze(1)\n#                 targets = data[\"target\"].to(device).unsqueeze(1)\n                targets = data[\"target\"].to(device, dtype = torch.long)\n                \n                random_seed()\n                val_outputs = model(imgs)\n#                 loss = loss_fn(val_outputs, targets.float())\n                loss = loss_fn(val_outputs, targets)\n#                 scheduler.step(loss)\n                \n                batch_losses_val.append(loss.item())\n                _, val_preds = torch.max(val_outputs, dim = 1)\n                n_correct_val += torch.sum(val_preds == targets) \n                \n                if epoch == epochs - 1:\n                    cm_targets.append(targets.cpu().detach().numpy())\n                    cm_preds.append(val_preds.cpu().detach().numpy())\n                \n        epoch_loss_val = np.mean(batch_losses_val)\n        \n        if epoch == epochs - 1:\n            # Store val_loss of last epoch to get final averaged loss per run???\n            # run loss\n            validation_loss += epoch_loss_val\n            \n        \n                \n        dt = time.time() - start_time\n#         lr = scheduler.get_last_lr()[0]\n        lr = optimizer.param_groups[0]['lr']\n        \n        print(f'Epoch {epoch + 1}/{epochs} \\t lr={lr:.1e} \\t t={dt:.0f}s \\t loss={epoch_loss:.4f}, acc={n_correct.double() / n_samples_train:.4f} \\t val_loss={epoch_loss_val:.4f}, val_acc={n_correct_val.double() / n_samples_val:.4f}')\n#         print(f'Epoch {epoch + 1}/{epochs} \\t t={dt:.0f}s \\t loss={epoch_loss:.4f}, acc={n_correct.double() / n_samples_train:.4f} \\t val_loss={epoch_loss_val:.4f}, val_acc={n_correct_val.double() / n_samples_val:.4f}')\n\n        if epoch_loss_val <= validation_loss_min:\n            print(f'Validation loss decreased. Saving model... ')\n            torch.save(model.state_dict(), f'model_{fold}.pt')\n            validation_loss_min = epoch_loss_val\n        \n    # Predict on test set \n    batch_outputs_test = []\n    \n    with torch.no_grad():\n        for batch_index, data in enumerate(test_loader, 0):\n            imgs = data[\"img\"][\"image\"].to(device)#.unsqueeze(1)\n#             imgs = data[\"img\"].to(device)#.unsqueeze(1)\n\n            random_seed()\n            test_outputs = model(imgs)\n            test_outputs = F.softmax(test_outputs, dim = 1)\n            test_outputs = test_outputs.cpu().detach().numpy()\n            batch_outputs_test.append(test_outputs)\n    \n    return validation_loss, np.vstack(batch_outputs_test), cm_targets, cm_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# SAVE BEST MODELS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1"},{"metadata":{"trusted":true},"cell_type":"code","source":"check data structures efficiency\nuse smaller model - resnet18?\nuse smaller iamge size for training and testing\nuse tpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train_new.label.value_counts().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0.2*2849, 0.8*2849 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save models!!!!\nTry 1 channel model input\nreduce epochs\nlr scheduler\nweight decay\nsmaller model - resnet18?\nimage transforms\nTestTA\n\n1 channel, no resize of images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model: xresnet18(pretrained=False)\nimg: 1 channel \nimg_sz: 128\nsplit: random (80/20)\noptim: Adam\nepoch: 15\nsched: Cosine decay","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing / (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"1109/6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_new.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import class_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch_toolbelt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.modules.loss import _Loss\nfrom typing import Optional\nfrom functools import partial\n\n\n\ndef softmax_focal_loss_with_logits(\n    output: torch.Tensor,\n    target: torch.Tensor,\n    gamma: float = 2.0,\n    reduction=\"mean\",\n    normalized=False,\n    reduced_threshold: Optional[float] = None,\n    eps: float = 1e-6,\n) -> torch.Tensor:\n    \"\"\"\n    Softmax version of focal loss between target and output logits.\n    See :class:`~pytorch_toolbelt.losses.FocalLoss` for details.\n    Args:\n        output: Tensor of shape [B, C, *] (Similar to nn.CrossEntropyLoss)\n        target: Tensor of shape [B, *] (Similar to nn.CrossEntropyLoss)\n        reduction (string, optional): Specifies the reduction to apply to the output:\n            'none' | 'mean' | 'sum' | 'batchwise_mean'. 'none': no reduction will be applied,\n            'mean': the sum of the output will be divided by the number of\n            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n            specifying either of those two args will override :attr:`reduction`.\n            'batchwise_mean' computes mean loss per sample in batch. Default: 'mean'\n        normalized (bool): Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n        reduced_threshold (float, optional): Compute reduced focal loss (https://arxiv.org/abs/1903.01347).\n    \"\"\"\n    log_softmax = F.log_softmax(output, dim=1)\n\n    loss = F.nll_loss(log_softmax, target, reduction=\"none\")\n    pt = torch.exp(-loss)\n\n    # compute the loss\n    if reduced_threshold is None:\n        focal_term = (1.0 - pt).pow(gamma)\n    else:\n        focal_term = ((1.0 - pt) / reduced_threshold).pow(gamma)\n        focal_term[pt < reduced_threshold] = 1\n\n    loss = focal_term * loss\n\n    if normalized:\n        norm_factor = focal_term.sum().clamp_min(eps)\n        loss = loss / norm_factor\n\n    if reduction == \"mean\":\n        loss = loss.mean()\n    if reduction == \"sum\":\n        loss = loss.sum()\n    if reduction == \"batchwise_mean\":\n        loss = loss.sum(0)\n\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FocalLoss(_Loss):\n    def __init__(\n        self, alpha=None, gamma=2, ignore_index=None, reduction=\"mean\", normalized=False, reduced_threshold=None\n    ):\n        \"\"\"\n        Focal loss for multi-class problem.\n        :param alpha:\n        :param gamma:\n        :param ignore_index: If not None, targets with given index are ignored\n        :param reduced_threshold: A threshold factor for computing reduced focal loss\n        \"\"\"\n        super().__init__()\n        self.ignore_index = ignore_index\n        self.focal_loss_fn = partial(\n            softmax_focal_loss_with_logits,\n#             alpha=alpha,\n            gamma=gamma,\n            reduced_threshold=reduced_threshold,\n            reduction=reduction,\n            normalized=normalized,\n        )\n\n    def forward(self, label_input, label_target):\n        num_classes = label_input.size(1)\n        loss = 0\n\n        # Filter anchors with -1 label from loss computation\n        if self.ignore_index is not None:\n            not_ignored = label_target != self.ignore_index\n\n        for cls in range(num_classes):\n            cls_label_target = (label_target == cls).long()\n            cls_label_input = label_input[:, cls, ...]\n\n            if self.ignore_index is not None:\n                cls_label_target = cls_label_target[not_ignored]\n                cls_label_input = cls_label_input[not_ignored]\n\n            loss += self.focal_loss_fn(cls_label_input, cls_label_target)\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_toolbelt.losses import FocalLoss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtoolbox.transform import Cutout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torchvision.transforms.Compose","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nrandom_seed()\n\nkfold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = SEED)\nsplits = list(kfold.split(df_train_new, df_train_new.label.values))\n\nrandom_seed()\nsss = StratifiedShuffleSplit(n_splits = 5, random_state = SEED, test_size = 0.15)\n\n# train_transforms = transforms.Compose([\n#     transforms.Resize((224, 224)),\n# #     Cutout(),\n# #             transforms.ColorJitter(contrast = params['contrast'], hue = params['hue'], brightness = params['brightness']),\n# #             transforms.RandomAffine(degrees = params['degrees']),\n# #             transforms.RandomResizedCrop(224),\n# #             transforms.RandomHorizontalFlip(p = 0.5 if params['h_flip'] else 0.0),\n# #             transforms.RandomVerticalFlip(p = 0.5 if params['v_flip'] else 0.0),\n#     transforms.ToTensor(),\n#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n# ])\n\n# test_transforms = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n# ])\n\n\ntrain_transforms = albumentations.Compose([\n    #299 - inception\n    albumentations.Resize(224, 224),\n#     Cutout(),\n#     albumentations.CenterCrop(152, 152, p = 1.0),\n#     albumentations.RandomBrightness(),\n#     albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n#     albumentations.JpegCompression(80),\n#     albumentations.HueSaturationValue(),\n#     albumentations.Normalize(mean=(0.5), std=(0.2)),\n    albumentations.Normalize(),\n    ToTensor()\n    ])\ntest_transforms = albumentations.Compose([\n    albumentations.Resize(224, 224),\n#     albumentations.CenterCrop(152, 152, p = 1.0),\n    \n#     albumentations.HorizontalFlip(),\n#     albumentations.RandomBrightness(),\n#     albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),\n#     albumentations.JpegCompression(80),\n#     albumentations.HueSaturationValue(),\n#     albumentations.Normalize(mean=(0.5), std=(0.2)),\n    albumentations.Normalize(),\n    ToTensor()\n    ])\n\nvalidation_losses_per_fold = []\ntest_predicitons_per_fold = []  \n\n# Folds\nfor fold in range(3):\n# for fold, (train_idx, valid_idx) in enumerate(splits):\n# for i, (train_idx, valid_idx) in enumerate(sss.split(df_train_new, df_train_new.target.values)):\n    # Data split\n#     train_set = df_train_new.iloc[train_idx].reset_index(drop = True)\n#     valid_set = df_train_new.iloc[valid_idx].reset_index(drop = True)\n    \n#     train_set = df_train_new.copy()\n    train_set = df_train_new[df_train_new.fold != fold + 1].reset_index(drop = True)\n    valid_set = df_train_new[df_train_new.fold == fold + 1].reset_index(drop = True)\n    \n#     train_targets = targets[train_idx]\n#     valid_targets = targets[valid_idx]\n    \n    test_set = df_test.copy()\n\n#     model = CustomNet()\n    random_seed()\n    model = ResNext(\"resnext50_32x4d\")\n#     model = get_model()\n#     model = get_model(\"dpn68b\")\n    model.to(device)\n#     weights = class_weight.compute_class_weight(\"balanced\", np.unique(train_set.target), np.asarray(train_set.target))\n#     loss_fn = torch.nn.CrossEntropyLoss(weight = torch.tensor(weights, dtype = torch.float32)).to(device)\n    loss_fn = nn.CrossEntropyLoss().to(device)\n#     loss_fn = nn.BCEWithLogitsLoss().to(device)\n#     loss_fn = LabelSmoothingLoss(classes = 193, smoothing = 0.1).to(device)\n#     loss_fn = FocalLoss().to(device)\n\n    random_seed(SEED)\n    # Process Data into format required by Transformer model\n    # rename to image_transforms_train\n#     train_data = SpectrogramData(train_set, train_transforms, \"train\")\n    random.seed(SEED)\n    train_data = MelspectrogramData(train_set, audio_transforms = get_audio_transforms(), image_transforms = train_transforms, mask_transforms = None, train = True, sr=None)\n#     valid_data = SpectrogramData(valid_set, test_transforms, \"train\")\n    # No need to pass audio augmentation to valid and test\n    random.seed(SEED)\n    valid_data = MelspectrogramData(valid_set, audio_transforms = get_audio_transforms(), image_transforms = test_transforms, mask_transforms = None, train = True, sr=None)    \n#     test_data = SpectrogramData(test_set, test_transforms, \"test\")\n    random.seed(SEED)\n    test_data = MelspectrogramData(test_set, audio_transforms = get_audio_transforms(), image_transforms = test_transforms, mask_transforms = None, train = False, sr=None)\n\n    print(f'Fold {fold + 1}')\n\n    # Train, evaluate, predict\n    validation_loss, test_prediciton, cm_targets, cm_preds = train_evaluate_predict(model, train_data = train_data, valid_data = valid_data, test_data = test_data, loss_fn = loss_fn, lr = 1e-3, epochs = 10, batch_size = 32, warm_up_prop = 0.1, device = device, n_samples_train = len(train_set), n_samples_val = len(valid_set), fold = fold + 1)\n    # Obtain validation result per fold\n    validation_losses_per_fold.append(validation_loss)\n    # Obtain test predictions per fold\n    test_predicitons_per_fold.append(test_prediciton)\n\n    \n\nprint(\"=\" * 100)\n# Print summary validation result of all runs\n\nprint(f'Total avg val_loss={np.mean(validation_losses_per_fold)}, S/Dev={np.std(validation_losses_per_fold)}')    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Total avg val_loss=1.515544268820021, S/Dev=0.028791058927899692","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.Size([64, 3, 224, 224])\ntorch.Size([64, 2048])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Total avg val_loss=1.464852121141222","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = pd.DataFrame()\ncm[\"true\"] = 0\ncm[\"pred\"] = 0\n\ncm_true = []\ncm_pred = []\n\nfor c, p in zip(cm_targets, cm_preds):\n    \n    for ci, pi in zip(c, p):\n        cm_true.append(ci)\n        cm_pred.append(pi)\n#         print(ci, pi)\n#         break\n\ncm.true = cm_true\ncm.pred = cm_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm.pred.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train.target == 180]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm[cm.true != cm.pred][cm[cm.true != cm.pred].duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(confusion_matrix(cm.true, cm.pred)).head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"array = [[33,2,0,0,0,0,0,0,0,1,3], \n        [3,31,0,0,0,0,0,0,0,0,0], \n        [0,4,41,0,0,0,0,0,0,0,1], \n        [0,1,0,30,0,6,0,0,0,0,1], \n        [0,0,0,0,38,10,0,0,0,0,0], \n        [0,0,0,3,1,39,0,0,0,0,4], \n        [0,2,2,0,4,1,31,0,0,0,2],\n        [0,1,0,0,0,0,0,36,0,2,0], \n        [0,0,0,0,0,0,1,5,37,5,1], \n        [3,0,0,0,0,0,0,0,0,39,0], \n        [0,0,0,0,0,0,0,0,0,0,38]]\ndf_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJK\"],\n                  columns = [i for i in \"ABCDEFGHIJK\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 10/10 \t lr=3.5e-07 \t t=13s \t loss=0.0218, acc=1.0000 \t val_loss=3.6866, val_acc=0.2568","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"1.5342, 1e-2lr linear\n1.6575","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SGD\nchange linear layer from convd to linear\nremove pooling from resnet?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 10/10 \t lr=1.5e-08 \t t=56s \t loss=0.0411, acc=0.9977 \t val_loss=1.4785, val_acc=0.6541\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 1/10 \t lr=2.8e-04 \t t=59s \t loss=4.8358, acc=0.0627 \t val_loss=4.9758, val_acc=0.0919\nEpoch 10/10 \t lr=1.5e-08 \t t=52s \t loss=0.0372, acc=0.9975 \t val_loss=1.4563, val_acc=0.7000\nEpoch 10/10 \t lr=1.5e-08 \t t=49s \t loss=0.0343, acc=0.9988 \t val_loss=1.4065, val_acc=0.7000\nEpoch 10/10 \t lr=1.5e-08 \t t=51s \t loss=0.0398, acc=0.9968 \t val_loss=1.2776, val_acc=0.7236\nTotal avg val_loss=1.3801320923699272, S/Dev=0.07528204777321587\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bsize, pooling, divide num_steps by 2, nn.Dropout2d, different lr per block","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 1/10 \t lr=2.8e-04 \t t=55s \t loss=4.8148, acc=0.0710 \t val_loss=4.9255, val_acc=0.1108\nEpoch 10/10 \t lr=1.5e-08 \t t=56s \t loss=0.0398, acc=0.9970 \t val_loss=1.4840, val_acc=0.6919\n\nEpoch 10/10 \t lr=1.5e-08 \t t=57s \t loss=0.0339, acc=0.9986 \t val_loss=1.4229, val_acc=0.6865\n\nEpoch 10/10 \t lr=1.5e-08 \t t=57s \t loss=0.0351, acc=0.9986 \t val_loss=1.3906, val_acc=0.6992\n\nTotal avg val_loss=1.4324917594591777, S/Dev=0.038739795067151125\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 1/10 \t lr=2.8e-04 \t t=49s \t loss=4.8367, acc=0.0671 \t val_loss=4.9538, val_acc=0.1027\n\nEpoch 10/10 \t lr=1.5e-08 \t t=47s \t loss=0.0292, acc=0.9988 \t val_loss=1.5414, val_acc=0.6676\n\nEpoch 10/10 \t lr=1.5e-08 \t t=46s \t loss=0.0328, acc=0.9982 \t val_loss=1.4389, val_acc=0.6946\n\nEpoch 10/10 \t lr=1.5e-08 \t t=49s \t loss=0.0326, acc=0.9986 \t val_loss=1.4261, val_acc=0.7154\nTotal avg val_loss=1.4687624904844496, S/Dev=0.05160340356769676\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"5epochs - 1.8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 1/10 \t lr=2.8e-04 \t t=97s \t loss=4.8123, acc=0.0659 \t val_loss=5.0199, val_acc=0.0892\nEpoch 10/10 \t lr=1.5e-08 \t t=79s \t loss=0.0329, acc=0.9988 \t val_loss=1.4908, val_acc=0.6838\nTotal avg val_loss=1.4321853187349107, S/Dev=0.043044599625798464","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg no, inception no, wideresnet no, se_resnext no","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dpn68\nEpoch 1/10 \t lr=1.0e-03 \t t=48s \t loss=4.8503, acc=0.0573 \t val_loss=6.6229, val_acc=0.0378\nEpoch 10/10 \t lr=0.0e+00 \t t=47s \t loss=0.0786, acc=0.9939 \t val_loss=1.7418, val_acc=0.6135\nTotal avg val_loss=1.7397140993012323, S/Dev=0.03671610167116252\n\ndpn68b\nEpoch 1/10 \t lr=1.0e-03 \t t=50s \t loss=4.8867, acc=0.0532 \t val_loss=6.6598, val_acc=0.0297\nEpoch 10/10 \t lr=0.0e+00 \t t=48s \t loss=0.0885, acc=0.9875 \t val_loss=1.7078, val_acc=0.6135\nTotal avg val_loss=1.6704864899317424, S/Dev=0.04781056522494794\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"8432\nEpoch 1/10 \t lr=1.0e-03 \t t=58s \t loss=4.6629, acc=0.0762 \t val_loss=5.3995, val_acc=0.0486 - amplitude\nEpoch 1/10 \t lr=1.0e-03 \t t=51s \t loss=4.7230, acc=0.0557 \t val_loss=6.8581, val_acc=0.0568 - power_to_db - 1.79cv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 1/10 \t lr=1.0e-03 \t t=52s \t loss=4.7910, acc=0.0491 \t val_loss=4.7730, val_acc=0.0702\nTotal avg val_loss=0.9016740989685058, S/Dev=0.08772016769278056\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sss = StratifiedShuffleSplit(n_splits = 10, random_state = SEED, test_size = 0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cv = df_train_new.copy()\ndf_cv[\"fold1\"] = 0\ndf_cv[\"fold2\"] = 0\ndf_cv[\"fold3\"] = 0\ndf_cv[\"fold4\"] = 0\ndf_cv[\"fold5\"] = 0\ndf_cv[\"fold6\"] = 0\ndf_cv[\"fold7\"] = 0\ndf_cv[\"fold8\"] = 0\ndf_cv[\"fold9\"] = 0\ndf_cv[\"fold10\"] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cv.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, (train_idx, valid_idx) in enumerate(sss.split(df_cv, df_cv.target.values)):\n    \n    df_cv[f'fold{i+1}'].iloc[train_idx] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cv.fold1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cv[df_cv.fold1 == 1].target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cv[\"n_train\"] = df_cv.iloc[:,3:].sum(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cv[df_cv.n_train  == 10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cv.head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 1/10 \t lr=1.0e-03 \t t=50s \t loss=4.8382, rmse=2.199590910098633 \t val_loss=6.0494, rmse=2.459562242320464\nEpoch 10/10 \t lr=0.0e+00 \t t=48s \t loss=0.0462, rmse=0.21495246869812332 \t val_loss=1.0655, rmse=1.032251696770324\nTotal avg val_loss=1.102843779987759, S/Dev=0.06317825833591366\n\nresnext50_32x4d, linear scheduler\n\nTotal avg val_loss=1.080268983046214, S/Dev=0.06510206615551202\nresnext50_32x4d, cosine with hard restarts scheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os.makedirs('Imgs/Train/', exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norm p cosine sched","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 1/10 \t lr=1.0e-03 \t t=42s \t loss=4.9287, rmse=2.220068314568483 \t val_loss=4.7213, rmse=2.172865924382936\nEpoch 10/10 \t lr=0.0e+00 \t t=41s \t loss=0.0253, rmse=0.15905603813180638 \t val_loss=1.0774, rmse=1.0379841625922157\n\nTotal avg val_loss=1.112252542707655, S/Dev=0.056559017776547184\n10 epochs 0.1warmup audio normalization0.5\nresnet34\n\nTotal avg val_loss=1.1690512484974331, S/Dev=0.07658209840529347\n8epochs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Epoch 1/10 \t lr=1.0e-03 \t t=39s \t loss=4.9151, rmse=2.217008767105064 \t val_loss=5.6620, rmse=2.379499066810739\nEpoch 10/10 \t lr=0.0e+00 \t t=38s \t loss=0.0188, rmse=0.1371805378332227 \t val_loss=1.1823, rmse=1.0873376067358445\n10 epochs 0.1\n\nTotal avg val_loss=1.1959205044640435, S/Dev=0.04434246501704043\nCPU times: user 28min 45s, sys: 2min 16s, total: 31min 1s\nWall time: 32min 4s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tp = np.mean(test_predicitons_per_fold, axis = 0)\n# tp = test_predicitons_per_fold[0]\ntp = pd.DataFrame(tp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss = pd.read_csv(\"../input/giz-data/giz_data/SampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tp.columns = df_train_new[[\"label\", \"target\"]].sort_values(\"target\").drop_duplicates().label.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.iloc[:,1:] = tp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.iloc[:,1:].sum(axis = 1).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.iloc[:,1:].max(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# F.softmax(torch.tensor([0.1, 0.2, 0.3, 0.4]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.to_csv(\"times_1p5_3rdchannel_1_479cv.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.Size([32, 64, 28, 28])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.tensor().view(-1, 64 * 4 * 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wd = MelspectrogramData(train_set, get_audio_transforms(), train_transforms, train = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in DataLoader(wd, batch_size = 32, shuffle = False, num_workers = 0):\n    # 1st batch\n    dd = i\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dd[\"img\"][\"image\"].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model(dd[\"img\"][\"image\"].to(device)).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input.size(0), -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.ones((32, 64, 28, 28)).view(-1, 64*28*28).shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}